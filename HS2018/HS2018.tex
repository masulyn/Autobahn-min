\newif \ifdraft \drafttrue
\documentclass[format=sigplan, review=true, 9pt]{acmart}
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote
                                % with conference information in first column

\settopmatter{printacmref=false}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{arrows.meta}

\definecolor{tuftsblue}{RGB}{72,145,206}
\newcommand{\SHOWCOMMENT}[1]{\ifdraft #1 \fi}
\newcommand{\ksf}[1]{\SHOWCOMMENT{{\color{tuftsblue} [K: #1]}}}

\newcommand{\cut}[1]{}
\newcommand{\acut}[1]{}

\newcommand{\appref}[1]{Appendix~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\tblref}[1]{Table~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\listingref}[1]{Listing~\ref{#1}}

\newcommand{\eg}{\textit{e.g.}}

%\newcommand{\scaption}[1]{\caption{\small{#1}}}
\newcommand{\scaption}[1]{\caption{#1}}
%\setlength{\abovecaptionskip}{1ex}

\newcommand{\hotspot}[0]{hot spot}
\newcommand{\hotspots}[0]{hot spots}
\newcommand{\coldspot}[0]{cold spot}
\newcommand{\coldspots}[0]{cold spots}
\newcommand{\hotspotcost}[0]{\textit{hotSpotThreshold}}
\newcommand{\unfit}[0]{unfit}
\newcommand{\dangerous}[0]{dangerous}
\newcommand{\useful}[0]{useful}
\newcommand{\useless}[0]{useless}
\newcommand{\Ao}[0]{\textsc{Autobahn 1.0}}
\newcommand{\At}[0]{\textsc{Autobahn 2.0}}
\newcommand{\fit}[0]{fit}
\newcommand{\preopt}[0]{pre-search}
\newcommand{\postopt}[0]{post-search}
\newcommand{\Preopt}[0]{Pre-search}
\newcommand{\Postopt}[0]{Post-search}
\newcommand{\absim}[0]{\textit{absenceImpact}}
\newcommand{\overallThreshold}[0]{\textit{overallThreshold}}
\newcommand{\nonterm}[0]{non-terminating}
\newcommand{\unimp}[0]{unimproved}



\newcommand{\preoptElim}[0]{45}
\newcommand{\preoptFewerBangs}[0]{87.79\%}
\newcommand{\preoptPerformance}[0]{0.69}
\newcommand{\AoPerformance}[0]{0.75}
\newcommand{\postBangs}[0]{93.8\%}
\newcommand{\postRatioWorse}[0]{33\%}

\begin{document}
\acmJournal{PACMPL}
\title{\At:\break Minimizing Bangs while Maintaining Performance}
\acmConference[Haskell'18]{11th ACM SIGPLAN International Haskell Symposium}{September 27-28, 2018}{St.Louis, MO, United States}
\author{Marilyn Sun}
\affiliation{Tufts University}
\email{marilyn.sun@tufts.edu}
\author{Kathleen Fisher}
\affiliation{Tufts University}
\email{kfisher@cs.tufts.edu}
\acmYear{2018}
\begin{abstract}

While lazy evaluation has many advantages, it can result in serious
performance costs. To alleviate this problem, Haskell allows users to
force eager evaluation at certain program points by inserting
strictness annotations, known and written as bangs (!).
Unfortunately, manual bang placement
is labor intensive and difficult to reason about. The \Ao{}
optimizer uses a genetic algorithm to automatically infer bang annotations
that improve runtime performance. However, \Ao{} often generates
large numbers of superfluous bangs,
which is problematic because users must inspect each such bang to
determine whether it introduces non-termination or other semantic
differences. 
This paper introduces \At, which uses GHC
profiling information to reduce the number of superfluous bangs.
Specifically,
\At{} adds a \textit{\preopt{} phase} before \Ao's genetic algorithm
to focus the search space and a \textit{\postopt{} phase} to individually test
and remove bangs that have minimal impact. 
\cut{When evaluated on the NoFib
benchmark, the \preopt{} phase on average eliminated 45~locations for
potential bang placement per 100~LOC and reduced the number of bangs
eventually generated by 12.21\%. Overall, }
When evaluated on the NoFib benchmark suite, 
\At{} reduced the number of inferred bangs by 90.2\% on average,
\cut{\At{} reduces the number of
bangs generated from 11~bangs/100~LOC to 1 bang/100~LOC, }
while only degrading program performance by 15.7\% compared with the
performance produced by \Ao{}. 
In a case study on a garbage collection simulator, 
\At{} eliminated 81.8\% of the recommended bangs,  with the 
same 15.7\% optimization degradation when compared with \Ao{}.
\acut{
 Finally, it eliminated 91.3\% bangs with almost no runtime slowdown on
the \texttt{Aeson} parser library.}
\end{abstract}
\maketitle

\section{Introduction}

\subsection{Lazy Evaluation}
Haskell's lazy evaluation strategy supports modularity, can improve
program efficiency, and enables the use of infinite data
structures~\cite{Hughes89}. Under lazy evaluation, expressions are
only evaluated when their values are needed. Every unevaluated
expression is stored in a \textit{thunk}, and its evaluation is
delayed until another expression demands the value of the current
one~\cite{PeytonJones89}. 

While lazy evaluation reaps many benefits, it can also cause serious
performance problems in both time and space when large amounts of
memory are allocated to thunks~\cite{Jones94,Santos98,Ennals03}.
Reducing the number of thunks created at runtime is an important
optimization in the GHC compiler, which uses a backward static
analysis~\cite{Sergey14} to statically find expressions that the
compiler can safely evaluate immediately rather than converting into
thunks.  Because this analysis is part of the GHC compiler, it must be
conservative, eliminating only thunks that it can prove will not
affect the program semantics when given any possible input.
Unfortunately, this conservative analysis is not always sufficient, in
which case programmers can manually insert strictness annotations such
as \textit{bang patterns}~\cite{bang}, which instruct the compiler to
immediately evaluate the corresponding expression regardless of
whether the strictness analysis determines it is safe to do so.  These
manual strictness annotations can significantly improve
performance~\cite[Chapter~25]{rwh}. However, programmers need to
distinguish program points that will benefit from eager evaluation
from program points that do not need to be evaluated or will not
terminate when evaluated. This task is difficult and often reserved
for expert Haskell programmers~\cite{Mitchell13}.

\subsection{\Ao}

\Ao~\cite{autobahn-wang} is a tool that helps Haskell programmers 
reduce their thunk usage by automatically inferring strictness
annotations. Users provide \Ao{} with an unoptimized
program, representative input, and an optional configuration file to
obtain an optimized version of the program over the course of a couple
of hours.
\Ao{} uses a genetic algorithm to randomly search for
beneficial locations to place bangs in the program. The genetic
algorithm iteratively measures the performances of a series of
candidate bang placements. Candidates that improve upon the original
program's performance are preserved, and candidates that trigger
non-termination or worsen program performance are
eliminated. \Ao{} eventually returns a list
of well-performing candidates, ranked by how much each candidate
improves program performance. Users can then inspect the candidate
bang placements and decide if they want to apply one of them to the
program.

\subsection{Too Many Bangs}

Users need to inspect candidate bang placements before applying them
to ensure the bangs maintain the desired semantics on all \textit{relevant}
inputs.  Because \Ao{} measures performance when the program is run on
the supplied input, the resulting annotations optimize the program
\textit{for that specific input}, which is why it is important for users
to provide representative input.  When run on different input, the
annotations could change the termination behavior of the program,
which may or may not be a problem.
We can run GHC's static analysis after
\Ao{} to check the safety of the inferred bangs; on average it
only marks 10\% of them as safe.  The remaining 90\% are either unsafe
on some inputs or their safety is unknown because of the
conservativity of the analysis.


In reality, bangs only need to be safe on inputs that the user will
actually pass to the program.  By analogy, we do not generally mind
that the usual factorial function diverges when passed a negative
number because we know the function is only intended to be applied to
postive numbers.  Because only the user knows the range of valid
inputs, only the user can inspect candidate bang placements to decide
if they are safe on all relevant inputs. However, users face a
time-consuming task when \Ao{} generates many bangs in a candidate
placement.  This task is particularly frustrating when some of the
bangs may not contribute much to program performance. On average,
\Ao{} generates 11~bangs per 100~lines of code in its best performing
candidates, and the user must manually inspect every one of those.

\subsection{\At{}}

This paper presents \At{}, an improved version of \Ao{} that aims
to reduce the number of generated bangs. 
\At{} introduces a \textit{\preopt{} phase} and
\textit{\postopt{} phase} that run before and after \Ao{},
respectively.  Both phases use information from GHC's profiler to
locate and eliminate ineffective bangs. 
GHC profiles are helpful in this regard because they
show the amount of runtime and memory each location in the
program is responsible for. The \preopt{} phase uses this information to adjust the set of files that
\Ao{} considers during its optimization.
Specifically, this phase instructs \Ao{} to
optimize files that contain locations that require significant
resources, skipping files that do not and
potentially adding files not originally included by the users.
After \Ao{} runs, the \postopt{} phase individually tests
each candidate bang that falls within a costly location. Bangs that
do not significantly impact program performance are
eliminated.   The system is parameterized, so users can manage the
tradeoff between aggressively reducing the number of bangs and
preserving performance improvements.  In our experiments, we chose to
aggressively reduce the number bangs, accepting some performance
degradation over the level of optimization provided by \Ao{}.

The contribution of this paper are the following:
\begin{itemize}
  \item We describe how to use profiling information to automatically reduce the number
    of bangs inferred by \Ao{} while maintaining roughly the same
    level of optimization.
  \item We show that \At{} applied to the NoFib benchmark suite reduced
    the number of generated bangs by 90.2\% on average, while
    increasing the runtime of the optimized program by 15.7\% over the
    runtime of the program optimized by \Ao{} alone. We refer to this
    performance change as a 15.7\% \textit{optimization degradation}.
\cut{(2.0 runtime - 1.0 runtime)/original}
  \item We demonstrate that the \preopt{} phase removed at least
    one file from consideration in 20 of the benchmarks in the NoFib
    suite, corresponding to 33.3\% of the programs we considered.
    For these programs, the \preopt{} phase eliminated
    45~potential bang locations per 100~LOC, resulting in a mean bang
    reduction of 87.79\% across the entire benchmark suite. 
  \item We use a microbenchmark to show that the \preopt{} phase's
    suggestions for additional files to consider can improve \Ao{}'s
    optimization results by 86.6\%.
  \item We evaluate the \postopt{} phase on the NoFib benchmarks,
    showing it can reduce the number of inferred bangs by
    93.8\% with a 33\% optimization degradation.
  \item We use \At{} in a case study to optimize the performance of 
    \texttt{gcSimulator}~\cite{Ricci13}, a garbage collector
    simulator. The system reduced the number of inferred bangs by
    81.8\% with a 15\% optimization degradation.
\acut{
  \item We apply \At{} in a second case study to show that it can
    preserve the application-specific annotations inferred by \Ao{}
    for two different uses of the Aeson~\cite{aeson} library.}
\end{itemize}


\section{Background}

\subsection{GHC Profiling and Cost Centers}
To focus the search on bangs that are likely to impact performance, we
make use of the information provided by GHC's profiling system.
This system allows users to better understand which locations
in a program consume the most resources.  The system adds annotations
to the program and generates a report detailing the amount of time,
memory allocations, or heap usage that is attributable to each location.
To generate these profiles, users simply run their program on
representative input after re-compiling it with options to request profiling.
Users can choose to generate either a time and allocation or a heap
profile, as well as the method by which the profiling system adds
annotations. Users can manually specify
annotations or invoke the profiler with  the \texttt{-prof -fprof-auto} option, which
automatically adds an annotation around every binding in the program
that is not marked \texttt{INLINE}.  In the generated profile, each
annotation gives rise to a \textit{cost center} that indicates how
much time or memory were attributable to the given program point as a
percentage of the whole program's resource utilization.  


\subsection{Genes and Chromosomes}

Cost center profiling provides guidance for the otherwise random
search that \Ao{} performs using a genetic algorithm. In the
algorithm, any program source location where a bang may be added is
represented as a \textit{gene} that can be turned \textit{on},
corresponding to the presence of a bang, or \textit{off},
corresponding to its absence.  Since it doesn't make sense to put two
bangs in one program location, there are a fixed number of possible
bang locations in a given program. A \textit{chromosome} comprises all
of the genes within a file. We represent a chromosome as a
fixed-length bit vector, in which each bit indicates the presence or
absence of a bang at the corresponding location. Since a program is a
collection of source files, it is represented as a collection of bit
vectors, or chromosomes.

\subsection{\Ao{}'s Genetic Algorithm}

\Ao{}'s genetic algorithm evaluates and manipulates randomly generated
chromosomes. It repeatedly generates new chromosomes before measuring
their performance using a fitness function. We call a chromosome that
either significantly slows down program performance or causes non
termination an \textit{\unfit{}} chromosome. If the fitness function
determines that a chromosome is \unfit{}, the chromosome is
immediately discarded. If the fitness function determines that a
chromosome behaved well, the chromosome is deemed \textit{\fit{}} and
kept for future rounds of generation.

For each round of chromosome generation, \Ao{} introduces randomness
by performing either a mutation or a crossover. A mutation flips a
gene in the chromosome whenever a randomly chosen floating point
number exceeds the \textit{mutateProb} threshold. A crossover combines
two chromosomes by randomly picking half of the genes from each parent
chromosome. For either of these random operations, \Ao{} uses a unique
number generator each time to guarantee randomness.

\subsection{Optimization Coverage}
\Ao{} uses program source files as the unit of granularity for 
the set of program locations to consider when inferring bangs.  By
default, \Ao{} optimizes all source code files in the program's
directory. However, users can specify a different set of files by
manually adding or removing file paths in the \Ao{} configuration.
We call the set of files considered in a given run of \Ao{}
its \textit{optimization coverage}.
\Ao{} does not infer bangs for external libraries
that are imported by the program, but users can manually add local
copies of the source code for such libraries to include them in the optimization process.


\subsection{Representative Input}
To run \Ao{}, users need to provide input on which to run their
program. The input should be short enough for \Ao{} to finish
execution in a reasonable amount of time while still be long enough
for it to measure noticeable time improvements if there are
any.  The input should also be representative of the data the users
intend to supply to the finished program so the system optimizes the
program for that kind of data.  It is this ability to optimize for
relevant input and not all input that gives bangs, whether added by
Autobahn or by the user, the ability to outperform GHC's optimizer. 



\section{\At{}}
\subsection{Why Too Many Bangs Are Generated}

The first step towards reducing the number of inferred bangs is to identify
categories of bangs and to hypothesize why each such category exists.
To that end, we introduce three categories of bangs:
a \textit{\dangerous{}} bang is one that can significantly
degrade program performance, including causing non-termination;
a \textit{\useless{}} bang neither improves nor degrades 
performance but is undesirable because programmers must waste
valuable time reasoning about its safety; finally,
a \textit{\useful{}} bang improves program performance.


An \unfit{} chromosome performs poorly as a whole, but it can contain
a mixture of \dangerous, \useless{}, and \useful{} bangs (By
definition, it must contain at least one \dangerous{} bang).
\Ao{} handles \unfit{} chromosomes by discarding them entirely.  It does not
attempt to identify the \dangerous{} bangs. Removing such bangs from
consideration, rather than simply discarding their chromosomes, would
be useful because otherwise they can reappear in later generations as
the result of a random mutation.

Fit chromosomes face a similar issue. \Ao{} handles \fit{} chromosomes
by preserving the entire chromosome, without separating the \useful{}
bangs from the \useless{} ones. This lack of discrimination is
problematic for two reasons. First, we might lose \useful{} bangs in
future generations because we cannot guarantee that they will be
preserved by the mutation and crossover operations of the genetic
algorithm.  Second, \useless{} bangs can survive if they are grouped
with  \useful{} ones. The accumulation of such \useless{} bangs
can dramatically increase the number of inferred bangs, leaving the
user with a substantial reasoning task.  As program source code
increases in size, the number of \useless{} bang positions also grows,
increasing the likelihood the genetic algorithm will infer and
preserve \useless{} bangs that happen to live on a chromosome
with \useful{} ones. 

\cut{
We hypothesize that \Ao{} generates large numbers of superfluous bangs
because it does not identify the different categories of bangs within
the same chromosome. 
The randomness of \Ao{}'s genetic algorithm means the search space is
never reduced in size.

The further addition of randomness means that the entire
chromosome is repeatedly searched as the search space is never
definitively reduced. Because there is a fixed number of genes in a
program, the search space for the genetic algorithm is also
equivalently fixed. Therefore, as the program source code increases in
size, the algorithm also generates significantly more bangs as
chromosomes increase in size.
}

\subsection{The Solution}

Precisely identifying which bangs are \useless{}/\useful{} is undecideable, but
we can use GHC profiling to make an educated guess. 
Intuitively, a bang
that appears in a cost center that uses many resources, which we 
call a \textit{\hotspot}, is more likely to be \useful{}, while one in a
cost center using few resources, which we call a \textit{\coldspot}, is likely to
be \useless{}.  Leveraging that intuition, \At{} preserves bangs that appear
in \hotspots{} while eliminating those in \coldspots.

We introduce the \hotspotcost{} parameter to determine
which cost centers to consider hot:  a cost center consuming more
than \hotspotcost{} resources is considered hot, while
one consuming fewer is cold.  
Currently, we set \hotspotcost{} 
to 6\% of the overall program runtime, although that threshold can
be adjusted. As the threshold increases, \At{} preserves fewer bangs
at the risk of greater degradations in program performance.


\cut{
isolate portions
of a chromosome by their individual contributions to program
performance. Cost centers not only break down a chromosome into
smaller portions by source code bindings, but their associated costs
also imply how likely a bang placement will affect program
performance. If executing code at a \hotspot{} occupied a significant
portion of the overall program runtime, then a bang-induced change in
performance at the \hotspot{} will likely significantly affect overall
runtime as well. }

\At{} uses GHC profiling information to reduce the
number of generated bangs in two different ways, corresponding to two
different phases in the optimization pipeline, which is shown in \figref{fig:pipeline}.  The first phase, which
we call
the \Preopt{} phase, eliminates
any chromosome corresponding to a program source code file that
contains only \coldspots{}. \At{} invokes \Ao{} only on chromosomes
corresponding to files that contain at least one \hotspot{}.  Removing
chromosomes that are unlikely to contain \useful{} bangs reduces the
size of the search space with little chance of inadvertently
eliminating good sets of annotations.  The smaller search space is
likely to make \Ao{} more effective in improving performance.  In
addition, the system will not infer bangs for any of the genes on the
eliminated chromosomes.  Since such bangs are very likely to have
been \useless{}, this step is very likely to reduce the number of
inferred \useless{} bangs.

The second phase, which we call the \Postopt{} phase, uses profiling
information is to categorize as likely \useless{}/\useful{} bangs that occur within \hotspots{} on
chromosomes that \Ao{} determines to be fit.  In a world with infinite resources,
we would test all combinations of such bangs and select the best one
because the effect of combining bangs is hard to predict.
Unfortunately, the exponential search space is too large to explore
exhaustively. Instead, we individually turn off one such bang at a
time, measure the resulting performance, and keep only those that
significantly impact performance.  The number of such bangs is
sufficiently small that we can test each in turn because the number of
bangs in \hotspots{} in a program is relatively small.

In the rest of this section, we explain \At{}'s optimization pipeline
in more detail.

\begin{figure}
\tikzset{%
  >={Latex[width=2mm,length=2mm]},
            base/.style = {rectangle, rounded corners, draw=black,
                           minimum width=4cm, minimum height=1cm,
                           text centered, font=\sffamily},
  activityStarts/.style = {base, fill=blue!30},
       startstop/.style = {base, text width=4cm, fill=red!30},
    activityRuns/.style = {base, fill=green!30},
         process/.style = {base, text width=3cm, fill=orange!15,
                           font=\ttfamily},
}

\begin{tikzpicture}[node distance=1.5cm,
    every node/.style={fill=white, font=\sffamily}, align=center]
  \node (preo)             [activityStarts]              {\Preopt{}};
  \node (user)     [process, left of=preo, xshift=-3cm]          {Original Program, Representative Input, Autobahn Configuration};
  \node (autobahn)      [activityStarts, below of=preo, yshift=-2.5cm]
                                                      {\Ao{}};
  \node (end)      [startstop, left of=autobahn, xshift=-3cm, yshift=1cm]
                                                       {If program is unsuitable for optimization: Alert user and end process};
                                                  
  \node (posto) [activityStarts, below of=autobahn, yshift=-2cm]
                                                    {\Postopt{}};     
  \node (endmin)      [startstop, below of=end, yshift=-1cm]
                                                       {If negligible optimization improvement: Alert user and end process};                                                       
   \node (result)     [process, left of=posto, xshift=-3cm, yshift=-1.5cm]          {Optimized and minimized program};                                                   

  \draw[->]             (preo) -- node[text width=4cm]
  					{Remove \coldspots{}, suggest external libraries, check if program is unsuitable for optimization}(autobahn);
  \draw[->]     (user) -- (preo);
  \draw[->]      (autobahn) -- node[text width=4cm]
  					{Find winning chromosome using genetic algorithms}(posto);
   \draw[->]      (preo) -- (end);
   \draw[->]      (posto) -- (endmin);
  \draw[->]      (posto) -- node[text width=4cm, xshift=-2cm, yshift=0.8cm] {Individually test each bang in winning chromosome}
                                   (result);
  \end{tikzpicture}
\scaption{\At{} optimization pipeline.}
\label{fig:pipeline}
\end{figure}


\subsection{\Preopt{} Profiling}

Just as manually reasoning about bang placement can be difficult, 
so too reasoning about which files to optimize can be challenging.
\At{} makes this process easier by leveraging information
in GHC profiles.  
To that end, the \preopt{} phase begins by generating a GHC time
and allocation profile for the unoptimized program by running it on 
user-provided representative input.
\At{} then sets the optimization coverage
for \Ao{} to be source files that contain at least one \hotspot{}. 
In addition, 
\At{} identifies library files that contain \hotspots{} and
suggests to users that they add local copies so the library source
files may be optimized as well. 
\Ao{} then optimizes the program as usual, except using a more
targeted set of files/chromosomes.
Note that this phase can both expand the search space, by identifying
library files that contain hotspots, as well as reduce it, by
identifying program source files with no \hotspots{}. 

\Preopt{} profiling offers three important benefits.
First, it can greatly reduce the number of bangs \Ao{} suggests by
limiting the search space to those program files that have a chance of
significantly impacting performance. This focusing reduces the
possibility of generating \useless{} or \dangerous{} bangs by
eliminating them from consideration
and
increases the chances of generating \useful{} ones by allowing more of
the relevant search space to be explored within a given time budget.
Second, if a \hotspot{} is located in an external library file,
the \preopt{} phase can identify the relevant files so they can be
included in the optimization process. 
Third, it identifies programs that are potentially unsuitable for
\Ao{} optimization. If a program contains a large number of cost
centers that all contribute minimally to program runtime, there may
not be any way to substantially improve program performance by adding
bang annotations. If the \preopt{} phase concludes that
a program only contains \coldspots{}, it will alert the user and
abort, saving the time and effort of running the remaining phases, 
which are unlikely to identify significant performance improvements.


It is worth noting that although the \preopt{} phase can reduce the
size of the search space, it does not reduce \Ao{}'s runtime.
\Ao{} will always search until exhausting its time budget. 
The reduced search space does allow \Ao{} to explore the space
more thoroughly, statistically speaking, potentially allowing it to 
find better results within the fixed time budget.


\subsection{\Postopt{} Bang Elimination}

After \Ao{} explores the search space and suggests a winning set of
chromosomes, \At{} uses GHC profiling information to eliminate any
bangs on those chromosomes that don't significantly contribute to
program performance. To that end, 
the \postopt{} phase decides that any gene that is off in the winning
chromosomes should not have a bang in the final recommendation.
It removes these genes from further consideration. 
The \postopt{} phase then maps each remaining gene, which must be
turned on, to its corresponding cost center.  It turns off all genes
that do not fall within \hotspots{}, deciding not to recommend bangs
for the corresponding locations on the theory that such bangs are
likely to be \useless{}.  It then removes these genes from further
consideration. 

The remaining genes are the interesting ones in that they both
contain a bang and fall within a \hotspot{}. These genes require
further testing because they may still be \useless{}, failing to
improve program performance even though they fall within a \hotspot{}.
\cut{If they are \useless{}, they should be discarded. }
There is usually a sufficiently small number of remaining genes that
are both turned on and within a \hotspot{} that it is possible to
measure their impact one by one. Specifically, 
the \postopt{} phase tests each such gene in turn, turning
it off while keeping all the remaining bangs on.  It 
then runs the resulting program and compares its performance to that
of the winning chromosomes as determined by \Ao{}.
If the absence of the bang slows down the program by the value of the \absim{}
threshold parameter, the \postopt{} phase deems the bang useful and
decides to keep it. 
Otherwise, the bang is deemed \useless{} and is
discarded. The \absim{} threshold is adjustable; we set it to
5\%. The \postopt{} phase repeats this process for every bang
that is both in the winning chromosomes and in a \hotspot{}. 
The minimization result is the combination of all the surviving
bangs.

It is possible that there are no surviving bangs at the end of testing, and that occurs when
there are no \hotspot{} bangs remaining after all the \coldspot{} bangs are eliminated. 
Another possibility is that the combination of \hotspot{} bangs prior to testing slows down 
the program runtime to equal to or more than the original runtime, in which case the \preopt{}
phase will remove all bangs and return the original program.

\subsection{Representative Input}
Just as with \Ao{}, the quality of the representative input impacts the quality of
\At{}'s performance because different inputs may
generate wildly different results in GHC profiles. Therefore, users
must carefully choose the representative input.


\section{Implementation}

\subsection{Program Architecture}
\At{} wraps \Ao{} with the  \preopt{} and \postopt{} phases as shown
in \figref{fig:pipeline}.  
Prior to running the genetic algorithm, the \preopt{} phase invokes
GHC's profiler on the unoptimized version of the user's program with
the \texttt{-prof -fprof-auto} option to auto-generate cost centers
around every non-inlined program binding.  It uses the supplied
representative data as input to generate the time and
allocation profile.  This baseline profile is only generated once, and
the rest of \At{} refers to the same profile for \hotspot{}
information throughout the entire optimization pipeline.
Depending on the where the \hotspots{} fall according to the profiler, 
the program's optimization coverage will either be
automatically reduced or manually expanded by the user.

Then \At{} reuses the existing implementation
of \Ao{}~\cite{autobahn-wang} to identify a winning set of 
chromosomes.
\Ao{} uses the \textit{haskell-src-exts}~\cite{langexts} parser to parse
source files and to identify genes.  
It then applies a genetic algorithm,
implemented using the \texttt{GA} Haskell library~\cite{genetic}
with a fitness function based on measured runtime performance, to search for the best performing chromosomes.

The \postopt{} phase eliminates bangs from the best-performing
chromosomes returned from \Ao{}.  It does so by re-running the program
once for each gene that both lies in a \hotspot{} and is turned on in
the winning chromosomes.  Specifically, it turns off the gene, runs
the program on the representative input, measures its performance, and
keeps any bang that impacts performance more than the \absim{}
threshold.  Similar to \Ao{}, the \postopt{} phase uses
the \textit{haskell-src-exts} library to parse the relevant source
files, set all the bangs appropriately, and then pretty-print the
modified source code to then be compiled by GHC and run on the
representative input. 

Note that this process does not require re-running the original
profile.  Instead, we reuse the profiling information calculated
during the \preopt{} phase.  Bangs in \hotspots{} are tested in order
of decreasing costs. While we recognize that the order in which we
test the bangs may affect the performance, it is too time consuming to
test the bangs in all possible orders.  For simplicity, we chose to
test each bang once in order of decreasing costs.

\cut{
\ksf{For your thesis or for the final version, it might be worth
playing around a bit to see if the order makes any difference.  For
example, do the results change substantially if you wonder them in
increasing order instead?   Or is it just random.}}

Finally, the \postopt{} phase returns the final combination of bangs
that have survived each round of testing. If \Ao{} failed to find 
chromosomes that improved program runtime by at least the
\overallThreshold{} parameter, then
the \postopt{} phase will return the unoptimized program.
We base this choice on the
theory that the relatively insignificant performance improvement
indicates users are better off keeping the original program rather
than having to reason about the safety of the inferred bangs. 
Currently, we set the \overallThreshold{} value to 6\%, but users
may change it.   We are working on releasing the code for \At{} and
expect it to be available in September.



\cut{
The resulting chromosome is further tested and reduced using GHC
profiling information in the \postopt{} bang reduction phase. After an
initial pass of elimination to get rid of all turned-off bangs and
bangs located in \coldspots{}, we individually test the impact of the
absence of a turned-on bang in each \hotspot{}. If a bang meets
the \absim{} threshold, it is kept in future rounds of testing and
will remain in the final combination of bangs for the optimized
program. If a bang does not meet the \absim{} threshold, it is removed
for future rounds of testing and will not appear in the final
combination.
}


\subsection{Running \At{}}

Users run \At{} the same way as they would run \Ao{}. They
provide a copy of their program source code, representative input,
and an optional configuration file. The \preopt{}
and \postopt{} phases typically require a negligible amount of time
compared with the time required to run \Ao{}.

If \At{} completes successfully, users can find the resulting source
code with the minimized bang annotations in the same project
directory as they would find the usual \Ao{} \texttt{survivor}
and \texttt{results} directories. If \preopt{} profiling detected that
the program is unsuitable for optimization, or if \Ao{} failed to
significantly optimize the program, then \At{} warns the user and
halts the optimization process. If external libraries could be added to
the optimization coverage to potentially improve the results,
the \preopt{} phase alerts the user and then continues to optimize as
normal.

\subsection{Source Locations}
\Ao{} uses a bit vector to represent the genes in a
chromosome, with each bit corresponding to a possible bang location in
a source file.  In \At{}, we modified this representation so that each
bit also indicates the cost center associated with the possible bang
location.   Cost centers are uniquely identified by the corresponding source
locations in source files.  Thus, we mapped each bit to its corresponding
source line. To turn the bangs in a \hotspot{} on or off, we can
traverse the bit-location vector and manipulate the bits that are
tagged with source lines that fall within the range of
that \hotspot{}'s source location.

\subsection{Removing Illegal Genes}
Both \Ao{} and \At{} use the \textit{haskell-src-exts} parser to add
and remove bangs.  
Unfortunately, the version of \textit{haskell-src-exts} parser that we use
incorrectly identifies the left-hand side of bindings within instance
declarations as potential locations to place bangs. For that reason,
\Ao{} did not optimize files that contain instance declarations. 
To eliminate this restriction, we instead removed any randomly
generated illegal bangs prior to each round of fitness evaluation in
the genetic algorithm. The rest of the genetic search runs
identically as before.

To keep track of whether a bang is legal, we used a
validity-indicating boolean vector to represent whether each gene in
the source code is legal. Prior to inserting bangs into a
program, \At{} would check the validity of a gene against the boolean
vector to make sure that the bang is located in a legal location.

Generically traversing the parser-generated AST using boilerplate code
fails to identify illegal genes, so we needed to manually traverse the AST
to construct the validity vector. As we traversed the AST, we kept
track of whether a left-hand side binding is within an instance
declaration. If so, then that binding is an illegal bang location and
is marked as \texttt{false} in the validity vector. All other
legal bang locations are marked as  \texttt{true}.

With this approach, \At{} successfully avoids inserting bangs into
illegal locations.  As a result, \At{} can now optimize files that
include instance declarations, which was not previously possible.

\section{Evaluation}

\subsection{Experimental Setup}

All versions of Autobahn were compiled using GHC
version \texttt{8.0.2}. The NoFib benchmarks were compiled with
GHC version \texttt{8.0.2} using NoFib's default flags, the flag
\texttt{-XBangPatterns}, and the NoFib flag that enables profiling. 
We discarded the benchmarks in the NoFib suite that failed to compile
or run out of the box. We also excluded the benchmarks that \Ao{}
refuses to optimize because they already have very fast runtimes. That
left 60 benchmarks on which we could run experiments.

To study runtime performance results, we report what we call
the \textit{runtime performance ratio}, which is the ratio of the
optimized program's runtime to the runtime of the original program.
A ratio that reflects a performance improvement of more than \absim{}
is considered successful
because such a ratio indicates the
optimized program performs substantially better than the original, while higher
ratios are failures.  A runtime performance ratio of 1 indicates that
the optimized version of the program has the same runtime as the
original version and is therefore \textit{\unimp{}}.  A runtime
performance ratio of 2 is a sentinel value indicating that the
``optimized'' version of the program is \textit{\nonterm{}}, by which
we mean it runs for more than a timeout threshold longer than the
unoptimized program.  Failures have the effect of leaving the program
unchanged as the proposed annotations are discarded in favor of the
original.

To calculate the performance of a particular benchmark using a
particular optimization system (\eg{}, \At{} or a 
restricted version using only a subset \At's phases), we ran the system under test on the benchmark 10
times to account for random fluctuations.
We scored each run by its runtime performance 
ratio. Note that each run of the benchmark can produce a different
result not only because of variations in machine load but also
because the search process is randomized.  Failed runs can be further
categorized into runs in which \Ao{} fails and runs in which
the \postopt{} phase of \At{}
fails. If a benchmark failed at the \Ao{} stage, then we say that its
runtime performance ratio for that run is 1 and it recommended
0~bangs. If a benchmark succeeded at the \Ao{} stage but failed
in the \postopt{} phase, then we say its runtime performance ratio is
1 (because we discard the optimization results) and the
number of inferred bangs is the number of bangs suggested by the \Ao{}
phase.  Finally, we computed the \cut{\ksf{harmonic}}average of the runtime
performance ratios and the \cut{\ksf{arithmetic}}average of the number of
recommended bangs.    

When we study the results of a particular optimization system on a
particular benchmark, we also categorize the results into one of three
groups: complete success, partial success, and complete failure.  A
benchmark completely succeeds if all of its runs are successful,
completely fails if all of its runs are failures, and partially
succeeds if there is a mixture of these outcomes.

Finally, to compute summary statistics across all the benchmarks, 
we calculate the \cut{\ksf{harmonic} mean} average of the runtime performance ratios
and the \cut{\ksf{arithmetic} mean} average of the total number of suggested bangs
across all 60 benchmark programs. 


\subsection{\Preopt{} Search Space Reduction}

To assess the impact of the \preopt{} phase, we study the number of
genes that the phase eliminated from (or added to) consideration by \Ao{}.
We compare the number of bangs that \Ao{} inferred when run with and
without the \preopt{} phase, calling the optimizer comprised of
the \preopt{} phase plus \Ao{} the \textit{\Preopt{} optimizer. } We
note in how many of the runs the \Preopt{} optimizer failed.  Finally,
we compare the runtime performance ratio for \Ao{} with the ratio for
the \Preopt{} optimizer.
To compute this data, we ran both the \Preopt{} optimizer and \Ao{} on
the 60 programs from NoFib benchmark suite.  We optimized all runs 
on runtime only, and we set both the \hotspotcost{} and \absim{} thresholds 
to 6\%.

The \preopt{} phase eliminated at least one file in 20 out of the 60
benchmark programs. 
The remaining 40 benchmarks fall into one of two categories.
The first category consists of 34 benchmarks that did not have any files
eliminated by the \preopt{} phase. The second category comprises
6 benchmarks that the \preopt{} phase determined were not suitable for
optimization because they had no significant \hotspots{}. 
Since \Ao{} and the \Preopt{} optimizer handle these 40 benchmarks
identically, we exclude them from the graphs reporting on the results
of the \preopt{} phase. We discuss the second category of benchmarks in 
\secref{sec:file-elim}. None of the 60 programs in the benchmark suite
had an external \hotspot{} not in the original optimization coverage.

\figref{fig:preopt-bangs} shows the results from the 20 benchmarks that
had at least one file eliminated during the \preopt{} phase. The
column \texttt{Eliminated Genes} gives the number of potential bang
locations that were eliminated before \Ao{} ran. 
The \texttt{Original Bangs} column records the number of bangs in the
original program, most of which were zero. 
The \texttt{Failed runs} column gives the fraction of the 10 runs on
which the \Preopt{} optimizer failed. 
Finally, the \texttt{\Ao{} Bangs} and \texttt{\Preopt{} Bangs} columns
give the number of recommended bangs by the two systems,
respectively.  On average, the \preopt{} phase eliminated
\preoptElim{} genes per 100 LOC across the 60 programs in the benchmark suite before
invoking \Ao{}.
The \Preopt{} optimizer recommended \preoptFewerBangs{} fewer bangs
than \Ao{}.  


The \texttt{anna}, \texttt{expert}, and \texttt{symalg} benchmarks are
particularly interesting because \Ao{} consistently failed to find
winning chromosomes for them, and so it did not generate any bangs. However,
after reducing the size of the search space, the \Preopt{}
optimizer was able to find meaningful bangs for them.

\figref{fig:preopt-runtime} shows the corresponding runtime
performance ratios produced by \Ao{} and the \Preopt{} optimizer on
the same 20 benchmarks.
The graph shows that even when a large number of genes are
eliminated prior to running the genetic algorithm, the optimizer is still able to
find \useful{} bangs that result in similar runtime improvement. This data
confirms that the eliminated genes were on the whole not \useful{}.
On average on these 20~programs, the \Preopt{} optimizer returns
runtime performance ratios of \preoptPerformance{} compared
to \AoPerformance{} for \Ao, which is an
optimization \textit{improvement}.  


\begin{figure*}
\includegraphics[width=\textwidth]{pre-aut-bangs}
\scaption{Number of bangs generated by \Ao{} vs. \Preopt{} optimizer across 20 benchmarks that had at least
one file eliminated during the \preopt{} phase. Columns that exceed
the maximum axis value are labeled with their actual values. The
benchmarks are sorted in increasing order of number of failed runs for
the \Preopt{} optimizer.}
\label{fig:preopt-bangs}
\end{figure*}

\begin{figure*}
\includegraphics[width=\textwidth]{pre-aut}
\scaption{Peformance runtime ratios of \Ao{} vs. \Preopt{} optimizer
across 20 benchmarks that had at least one
file eliminated during the \preopt{} phase. The x-axis 
is on a base-2 log scale. The benchmarks appear in the same order as
in \figref{fig:preopt-bangs}.}
\label{fig:preopt-runtime}
\end{figure*}

\subsection{\Preopt{} File Elimination}
\label{sec:file-elim}

The preopt{} phase identifies 6 benchmarks among the 60 in our suite
as unsuitable for optimization when the \hotspotcost{}
threshold is set to 6\%:  \texttt{awards}, \texttt{callback001},
\texttt{callback002}, \texttt{mutstore2}, \texttt{sorting}, and \texttt{threads007}. As
expected, when attempting to
optimize \texttt{awards}, \texttt{sorting},
and \texttt{threads007}, \Ao{} consistently fails, returning
the \unimp{} runtime code. However, \Ao{} was able to
successfully optimize the other programs. Through inspection, we
concluded that \texttt{callback002} would have benefited from a
lower \hotspotcost{} threshold as its most costly hot spot takes up
3.9\% of the program runtime. Both \texttt{callback001}
and \texttt{threads007} would have benefited from inspecting heap
profiles instead of time and allocation profiles as the costs
associated with their hot spots were noticeably larger in heap
allocations while remaining insignificant in runtime
costs. The \texttt{mutstore2} program's performance fluctuated wildly
even without bangs in it. For example, its measured runtime was as low
as 60\% - 80\% of its original runtime in one-third of the experiments
we ran with no bangs in the program. Therefore, the optimization
results were likely skewed by the fluctuating runtime.

\subsection{\Preopt{} File Addition}
To demonstrate the effectiveness of using the \preopt{} phase to
expand Autobahn's coverage for improved optimization results, we
tested our approach on the \texttt{sumList} microbenchmark. We
created \texttt{sumList} to simulate the scenario in which a
programmer references code from an external library or external file
that contains \hotspots{} but is not within the current optimization
coverage. 

The \texttt{sumList} program's \texttt{Main.hs} file contains only one
function: a main function that constructs a list of integers from 1 to
1,000,000 and then calculates the sum of all integers in the list using an
external \textit{sum} function located in \texttt{Sum.hs}. Users
may decide to set the optimization coverage to [\texttt{Main.hs}],
because they are interested in making the main program run
faster. However, as demonstrated in \figref{fig:sumList}, \Ao{} was only able to
improve program performance by 3\%, even when it was able to
exhaustively search the possible bang locations in the 6 lines of code in the \texttt{main}
function. Upon inspecting the results, users may be mistaken in
believing that their program runtime cannot be improved
further.

But there are indeed other opportunities to speed up \textit{sumList}
located in places that the users did not think about. If the users were
to rerun the optimization using the \preopt{} phase, then GHC's time and
allocation profile indicates that the largest \hotspotcost{} was 9.5\%
and located in lines 7 to 8 in the \textit{sum} function
in \texttt{Sum.hs}. The \textit{sum} function is entirely lazy and did
not immediately compute the sum of each integer as it recursed through
the list. In the resulting log file, the \preopt{} phase 
suggests 
adding \texttt{Sum.hs} to the optimization coverage.
After expanding the coverage and re-running the optimization,
the resulting \texttt{sumList} ran at only 13\% of the original
runtime, a dramatic improvement.

Although the \texttt{sumList} example is short and synthetic, it shows
the larger potential for users to obtain much better optimization
results when running the \preopt{} phase in conjunction
with \Ao{}. Programmers often build upon each other's code and use
external functions that they may not be entirely familiar with or did
not consider optimizing. The \preopt{} phase can identify valuable
missed opportunities. Of
course, the addition of more files to optimize means that more bangs
might be generated. It is up to the user to decide if they want to add
the suggested files for better optimization results at the risk of
needing to inspect more bangs.
\newline
%We also ran experiments using the Aeson parser library that handles JSON files using either the \texttt{validate} or \texttt{convert} driver programs. \texttt{validate} simply checks if the file is written in valid JSON syntax, and \texttt{convert} actually converts file input into a Haskell data structure. 

%While running the \preopt{} phase on the Aeson library, we were suggested by the program to include files from the external \texttt{Data/Aeson} library in the Autobahn optimization coverage because the \texttt{Data/Aeson/InternalTypes.hs} file included multiple \hotspots{}. However, \texttt{InternalTypes.hs} already included manually inserted bangs by its author. Therefore we ran experiments using a version of \texttt{InternalTypes.hs} with no bangs in it to see if we could recreate the manually inserted bangs. If so, library authors could run Autobahn to place bangs in their files instead of manually doing so. 


\begin{figure}
\begin{tabular}{p{2.5cm}p{1.5cm}p{2cm}p{1cm}}
\hline
Version   & Coverage & Normalized Runtime & No.Bangs \\
\hline
Original      & N/A   &   1	 & 0   \\
\Ao{}       & [\texttt{Main.hs}]      & 0.97    &  2\\
Pre-optimization	& [\texttt{Main.hs}, \texttt{Sum.hs}]         & 0.13      & 4\\
\hline
\end{tabular}
\scaption{Results for \texttt{sumList} microbenchmark.}
\label{fig:sumList}
\end{figure}
%\begin{tabular}{llr}
%\hline
%Version   & Driver & Normalized Runtime \\
%\hline
%Original      & value   & 1     \\
%          & json        & 1      \\
%Pre-optimization       & value     & 0.73     \\
%          & json        & 0.66	\\
%\Ao{}       & value     & 0.56      \\
%          & json        & 0.72	\\
%\hline
%\end{tabular}

\subsection{\Postopt{} Bang Reduction}

To assess the effectiveness of the \postopt{} phase,
we compare the results of running \Ao{} with running 
the \textit{\Postopt{} optimizer} comprised of \Ao{} followed by
the \postopt{} phase.
\cut{
Already discussed and so not necessary:
Similarly, we took the mean of running the
program ten times on the NoFib benchmark suite while optimizing on
runtime only, and set both \hotspotcost{} and \absim{} thresholds to
6\%.
A benchmark is successfully optimized if \Ao{} improved its
performance by at least 6\% after optimization. }
Figures~\ref{fig:post-bangs-all} and \ref{fig:post-ratio-all}
give the number of bangs and the runtime performance ratios,
respectively, for the 21 benchmarks that \At{} successfully optimized on all 10 runs. 
Figures~\ref{fig:post-bangs-some} and \ref{fig:post-ratio-some}
give the number of bangs and the runtime performance ratios, respectively,
for the 28 benchmarks on which \At{} was partially successful,
sorted in increasing order of \At{}'s failure rate.
We do not show the results from the remaining 12 benchmarks that \At{}
failed to optimize on every run since the numbers for those benchmarks
would be unchanged from the original program.  \figref{fig:post-failures} shows how
frequently a benchmark failed at the \Ao{} stage vs. how frequently it
failed at the \postopt{} phase. The graph shows that the majority of \Postopt{} optimizer fails are
 due to failure at the \Ao{} stage, which produces a poor performing set of bangs for the \postopt{} phase
 to minimize on. Therefore, benchmarks that failed at the \Ao{} stage are highly likely to fail during 
 the \postopt{} phase as well.


Figures~\ref{fig:post-bangs-all} and \ref{fig:post-bangs-some} show
that the number of bangs eliminated by the \Postopt{} optimizer is quite
significant: on average \postBangs{}. Figures~\ref{fig:post-ratio-all}
and~\ref{fig:post-ratio-some} show the corresponding runtime
performance ratios of each optimized program. In most benchmarks,
the \Postopt{} optimizer does
a little worse than \Ao{}, on average \postRatioWorse{} worse, because 
the \postopt{} phase only preserves bangs that affect program
runtime by at least the \absim{} threshold (6\% in these experiments). 
If users want to
maintain higher levels of optimization,
they can lower the \absim{} threshold so the minimizer becomes less
aggressive in bang elimination. That way, more bangs will be
preserved, but runtime performance will
improve. 

Looking at these results in more detail, 
the \texttt{callback001} and \texttt{pic} benchmarks are good
examples of when a program has no cost centers that meet
the \hotspotcost{} threshold, so the \postopt{} phase eliminated all their bangs.
In these cases, a lower threshold would have
helped discover useful bangs that are located in the relatively colder locations.
The data for \texttt{anna} and \texttt{fluid}
show that while \Ao{} found bangs that triggered a \nonterm{} runtime
code, \postopt{} bang elimination was able to eliminate
the \dangerous{} bangs that caused the bad behavior and instead produce
successful optimization results.

\begin{figure*}
\includegraphics[width=\textwidth]{aut-post-bangs}
\scaption{Number of bangs generated by \Ao{} vs. \Postopt{} optimizer 
across 21 benchmarks that \Ao{} successfully optimized
every time. Columns that exceed the maximum axis value are labeled
with their actual values. }
\label{fig:post-bangs-all}
\end{figure*}

\begin{figure*}
\includegraphics[width=\textwidth]{aut-post}
\scaption{Performance runtime ratios of \Ao{} vs. \Postopt{} optimizer
across 21 benchmarks that \Ao{} successfully
optimized every time. The x-axis is on a base-2 log scale.
For the *-ed benchmarks, \Ao{} generated non-terminating
optimization results, but the \postopt{} phase removed
the \dangerous{} bangs and succeeded in optimizing the program. }
\label{fig:post-ratio-all}
\end{figure*}

\begin{figure*}
\includegraphics[width=\textwidth]{ap-partial-bangs}
\scaption{Number of bangs generated by \Ao{} vs. \Postopt{} optimizer
across 27 benchmarks that \Ao{} successfully optimized on some runs. Failure rate
out of 10 runs is shown. Columns that exceed 
the maximum axis value are labeled with their actual values. 
The benchmarks are sorted in increasing order of \At{} failure rate. } 
\label{fig:post-bangs-some}
\end{figure*}

\begin{figure*}
\includegraphics[width=\textwidth]{ap-partial}
\scaption{Performance runtime ratios of \Ao{} vs. \Postopt{} optimizer
across 27 benchmarks that \Ao{} successfully optimized on some runs.
The x-axis is on a base-2 log scale. 
}
\label{fig:post-ratio-some}
\end{figure*}

\begin{figure*}
\includegraphics[width=\textwidth]{aut-post-fail}
\scaption{Frequency of \Ao{} failure vs \At{} failure across 27
benchmarks on which optimization sometimes succeeded. Benchmarks are sorted in
increasing order of \Preopt{} optimizer failure rate. } 
\label{fig:post-failures}
\end{figure*}

\subsection{\At{}: Combining \preopt{} and \postopt{}}

To evaluate the overall effectiveness of \At{}, we ran the complete
tool chain on our NoFib benchmarks. Out of those 60
programs, the \preopt{} phase eliminated 5 because they had
no \hotspots{}.  On another 3 benchmarks, neither \Ao{} nor \At{} 
successfully found an optimization on any run. 
It is interesting to note that only these 8
benchmarks consistently failed under \At{}, while 12
consistently failed under the \Postopt{} optimizer. 
This observation shows that the \preopt{} phase successfully
increased the effectiveness of the later optimization phases. 

Figures~\ref{fig:2-bangs-26}, \ref{fig:2-ratio-26}, \ref{fig:2-bangs-52},
and \ref{fig:2-ratio-52} presents the numbers of recommended bangs and
the runtime performance ratio on the 52 benchmarks that \At{} 
successfully optimized at least once.
The benchmarks are sorted in increasing order of \At{} failure rate.
We split the data into two graphs of 26 benchmarks each for bang counts
(Figures~\ref{fig:2-bangs-26} and~\ref{fig:2-bangs-52})
and for runtime performance ratios
(Figures~\ref{fig:2-ratio-26} and~\ref{fig:2-ratio-52})
for legibility. 
Finally, \figref{fig:2-failures} shows how frequently a benchmark
failed at the \Ao{} stage versus at the end of \At{}. Overall, \At{}
reduced the number of bangs generated by 90.2\%
with an optimization degradation of 15.7\%.


While most benchmarks consistently showed significant bang reduction
with minimal optimization degradation under \At{}, a few benchmarks stand
out. The \texttt{expert} and \texttt{calendar} benchmark not only had
bangs reduced by 79.41\% and 97.63\% respectively, but also
experienced performance \textit{improvements} of 27.59\% and 14.82\%
respectively. It is worth noting that both benchmarks failed
more times than they succeeded, so such results are not guaranteed to
be replicable in every run. The \texttt{atom} benchmark is also noteworthy
because the \postopt{} phase eliminated all bangs
generated by \Ao{}, yet it still had a runtime performance ratio of
0.78. This finding suggests that \texttt{atom}'s original
runtime fluctuates by a significant amount on its own. It also
suggests that \texttt{atom}'s overall performance
improvement is achieved through the accumulation of speedups at many
relatively cold cost centers, so lowering \At{}'s \hotspotcost{} 
might result in a better performance improvement.



\begin{figure*}
\includegraphics[width=\textwidth]{pap0-bangs}
\scaption{Number of bangs generated by \Ao{} vs. \At{} across the first
26 of 52 benchmarks that \At{} successfully optimized
at least once. Failure
rate out of 10 runs is shown. Benchmarks are sorted in increasing order of
failure rate. Columns that exceed the maximum axis value are labeled
with their actual values.}
\label{fig:2-bangs-26}
\end{figure*}

\begin{figure*}
\includegraphics[width=\textwidth]{pap0}
\scaption{Runtime performance ratios of \Ao{} vs. \At{} across
the first 26 of 52 benchmarks that \At{} successfully optimized at least
once. The x-axis is on a base-2 log scale.} 
\label{fig:2-ratio-26}
\end{figure*}

\begin{figure*}
\includegraphics[width=\textwidth]{pap1-bangs}
\scaption{Number of bangs generated by \Ao{} vs. \At{} across the
remaining 26 of 52 benchmarks that \At{} successfully optimized at least
once. Failure rate out of 10 runs is shown. Benchmarks are sorted in
increasing order of failure rate. Columns that exceed the maximum axis
value are labeled with their actual values. } 
\label{fig:2-bangs-52}
\end{figure*}

\begin{figure*}
\includegraphics[width=\textwidth]{pap1}
\scaption{Runtime performance ratios of \Ao{} vs. \At{} across the
remaining 26 of 52 benchmarks that \At{} successfully optimized at least
once. The x-axis is on a base-2 log scale.
}
\label{fig:2-ratio-52}
\end{figure*}

\begin{figure*}
\includegraphics[width=\textwidth]{pap-fail}
\scaption{Frequency of \Ao{} failure vs \At{} failure across the 36
benchmarks that \At{} sometimes successfully optimized. Benchmarks are sorted in
increasing order of \At{} failure rate. } 
\label{fig:2-failures}
\end{figure*}

\subsection{Case Study: \At{} On \texttt{gcSimulator}}
As a case study, we ran \At{} on \texttt{gcSimulator}, which is a
garbage collection simulator that consumes program execution traces
produced by the Elephant Tracks~\cite{Ricci13} monitoring system.  The
simulator comprises 2026~lines of code spread across 20~source code
files.  To keep \Ao{} runtime within reasonable ranges, we used the
first 1M of the \texttt{batik} trace file from the DaCapo
benchmarks~\cite{Blackburn06} as the representative input.  For this
case study, we lowered the \absim{} threshold to 1\%
because \texttt{gcSimulator} does not have many \hotspots{} at higher
thresholds.  Once \At{} was done optimizing, we evaluated the
resulting program on larger trace file sizes of 100M and 500M. Because
running the simulator on the full 6184M trace size took too
long with or without optimization, we did not record results for the
full trace.

\Ao{} produces results that run
faster on representative input as well as on larger trace files.
\At{} was  able to generate similar performance results with many fewer
bangs (125 vs. 690), as demonstrated in \figref{fig:gc}.

\begin{figure}
\begin{tabular}{lllr}
\hline
Version   & File Size (M) & Runtime & No.Bangs \\
\hline
Original      & 1   &   0.40	 & 0   \\
          & 100        & 43.13      & 0 \\
       & 500     &  216.71 & 0 \\
\Ao{}       & 1     & 0.18    &  690\\
          & 100        & 14.19 &  690\\
                 & 500        & 68.98	& 690\\
\At{}      & 1   &  0.23 & 125    \\
          & 100        & 15.75 & 125      \\
       & 500    & 81.66 & 125    \\

\hline
\end{tabular}
\scaption{Performance results for \texttt{gcSimulator}.}
\label{fig:gc}
\end{figure}

\acut{
\subsection{Case Study: \At{} with Aeson Library}
\ksf{We could also consider deleting this section if we don't have
       enough space.} 

The original experiments\cite{autobahn-wang} showed that \Ao{} was
able to infer application-specific bang patterns for two different
uses of the \texttt{Aeson} parser library for \texttt{JSON} files.
The \texttt{validate} driver checks if the input file is a
valid \texttt{JSON} file, which does not require completely evaluating
each \texttt{JSON} value in the file.  The \texttt{convert} driver
converts the \texttt{JSON} file into a Haskell data structure, and it
does need to completely evaluate each \texttt{JSON} value. If
the \texttt{Aeson} parser is paired with the \texttt{validate} driver,
it performs best when it evaluates its parsing functions
lazily. If \texttt{Aeson} is paired with the \texttt{convert} driver,
it performs best when it evaluates its parsing functions eagerly.  In
the experiments, the drivers were paired with library code that had
the wrong bangs pre-inserted to allow room for improvement.  \Ao{} did
correctly suggest the appropriate bang annotations for both drivers, in
one case inserting bangs and in the other removing them.

To explore whether \At{} preserves these results, we ran \At{} with
identical experimental conditions.
In the initial run, the \preopt{} phase identified
that \texttt{Aeson}'s top hot spots were in fact located in external
libraries. We followed the suggestions to
include \texttt{Data.Attoparsec.Internal.Types} as a local file
in \Ao{}'s optimization coverage. However, further experimentation
showed that including this file did not significantly improve \Ao{}'s
optimization ability, so we reverted to the original configuration.
We subsequently lowered the \hotspotcost{} threshold to 0.2\% because
otherwise most of the \hotspots{} were located in external libraries.

In subsequent runs, \Ao{} did not generate the exact
application-specific bang patterns for the two drivers seen earlier,
even when run without the \preopt{} phase.  The underlying genetic
algorithm is random, so each run can produce different results.  Since
our interest was partly whether the \postopt{} phase preserves
application-specific bang patterns inferred by \Ao{} while reducing
the number of \useless{} bangs, we manually inserted the desired bangs
into the \Ao{} ouput and proceeded with the \postopt{} phase.
With this input, the \postopt{} phase of 
successfully identified the key parsing function as a \hotspot{}
and preserved the bang pattern in the parsing function. Therefore, 
if \Ao{} had produced the correct bang patterns, \At{} would have
preserved them. 
\figref{fig:aeson} shows the resulting performance and bang counts.
Because the \At{} data reflects manually inserting a
bang, we include the same edits in the data associated with \Ao{}.
Essentially, the data reflects what would have been the outcome if
the \Ao{} phase had successfully identified the key annotations. 

%\begin{tabular}{p{2.5cm}p{1.5cm}p{1cm}p{1.5cm}}
%\hline
%Version   & Coverage & Driver & Normalized Runtime \\
%\hline
%Original      & N/A & convert   & 1     \\
%          & N/A & validate        & 1      \\
%\Ao{}  &[\texttt{Main.hs}]      & convert     & 0.56      \\
%          &[\texttt{Main.hs}] & validate        & 0.72	\\          
%\preopt{} and \Ao{}    &  [\texttt{Main.hs}, \texttt{InternalTypes.hs}]  & convert     & 0.73     \\
 %         & [\texttt{Main.hs}, \texttt{InternalTypes.hs}]  & validate        & 0.66	\\
%\hline
%\end{tabular}



\begin{figure}
\begin{tabular}{p{2.5cm}p{1.5cm}p{1.5cm}p{1.5cm}}
\hline
Version   & Driver & Runtime Ratio & No. Bangs\\
\hline
Original      & \texttt{convert}   & 1     &  2\\
              & \texttt{validate}  & 1     &  4\\
\Ao{}*        & \texttt{convert}   & 0.91  & 46\\
              & \texttt{validate}  & 0.86  & 93\\
\At{}*        & \texttt{convert}   & 0.93  &  7\\
              & \texttt{validate}  & 0.85  &  2\\
\hline
\end{tabular}
\scaption{Optimizing two driver programs for
the \texttt{Aeson} library, with the output of the genetic algorithm 
modified in both \Ao{} and \At{} to include the key bang patterns
found earlier~\cite{autobahn-wang} (Hence the asterisks).} 
\label{fig:aeson}
\end{figure}
}

\section{Related Work and Future Work}

\subsection{\Ao{} and Other Methods of Removing Laziness}

The current strictness analyzer in GHC uses backward abstract
interpretation to identify locations that can be eagerly
evaluated~\cite{Sergey14}. The analysis is sound but necessarily
approximate because it is static and the underlying property is
undecidable. The analysis only marks locations as strict if it can
guarantee termination on all possible inputs, not just realistic ones,
since it is part of the compiler.  Like \Ao{}, \At{} leverages the
advantages of being dynamic and not attempting to guarantee
termination on all inputs.  Instead, \Ao{} allows users to decide if
the annotations are safe on the intended inputs.

Other approaches for reducing laziness include Strict
Haskell~\cite{strict-haskell}, which allows users to make entire modules strict rather than
lazy by default using the \texttt{-XStrict} and \texttt{-XStrictData} language
pragmas. Chang and Felleisen~\cite{Chang14} take the opposite
approach: they start with a program written in a strict
language and inserts laziness annotations using dynamic
profiling. It would be interesting to see if Chang and Felleisen's
method could be applied to introduce laziness to Strict Haskell
programs.

\subsection{\At{} Improvements}
For future developments, it would be worth exploring the additional
use of heap profiles to locate hot spots instead of solely using time
and allocation profiles. We have occasionally seen programs in our
experiments that may be more accurately profiled by heap usage instead
of time and allocation profiles. Although it may be difficult to
predict which programs' \hotspots{} are more prominently associated
with heap usage, \At{} could run both profiling systems for a program
and compare the profiles before selecting the more suitable one to
use. Another potential improvement is implementing thresholds
that automatically adjust themselves based on profiling results. Our
experiments show that the ideal values for \hotspotcost{} and \absim{}
thresholds vary by program to program. Adopting more flexible
thresholds that automatically adjust themselves after inspecting the
profile in the \preopt{} phase might yield better results than using
set values or asking users to provide them.


\section{Conclusion}

Laziness is a double-edged sword: While it provides many benefits,
excessive laziness can cause poor performance. Strictness annotations
allow programmers to force eager evaluation, but its use is limited to
experienced programmers with high levels of expertise. \Ao{} uses a
genetic algorithm to automatically infer annotations to improve
program performance, but it often suggests too many bangs for users to
inspect. We have built \At{}, which uses GHC profiling feedback to
focus the search space and eliminates \useless{} bangs in a
post-processing phase.  Experiments show
that \At{} removes 90.2\% of bangs that \Ao{} recommended with only a
15.7\% optimization degradation on the NoFib benchmark, 
and 81.8\% of the bangs with the same 15.7\% optimization degradation 
on the \texttt{gcSimulator} case study.

\bibliographystyle{abbrvnat}
\bibliography{autobahn}

\end{document}
