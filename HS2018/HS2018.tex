\newif \ifdraft \drafttrue
\documentclass[format=sigplan, review=true]{acmart}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{arrows.meta}

\definecolor{tuftsblue}{RGB}{72,145,206}
\newcommand{\SHOWCOMMENT}[1]{\ifdraft #1 \fi}
\newcommand{\ksf}[1]{\SHOWCOMMENT{{\color{tuftsblue} [K: #1]}}}

\newcommand{\cut}[1]{}

\newcommand{\appref}[1]{Appendix~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\tblref}[1]{Table~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\listingref}[1]{Listing~\ref{#1}}

\newcommand{\eg}{\textit{e.g.}}

\newcommand{\hotspot}[0]{hot spot}
\newcommand{\hotspots}[0]{hot spots}
\newcommand{\coldspot}[0]{cold spot}
\newcommand{\coldspots}[0]{cold spots}
\newcommand{\hotspotcost}[0]{\textit{hotSpotThreshold}}
\newcommand{\unfit}[0]{unfit}
\newcommand{\dangerous}[0]{dangerous}
\newcommand{\useful}[0]{useful}
\newcommand{\useless}[0]{useless}
\newcommand{\Ao}[0]{\textsc{Autobahn 1.0}}
\newcommand{\At}[0]{\textsc{Autobahn 2.0}}
\newcommand{\fit}[0]{fit}
\newcommand{\preopt}[0]{pre-search}
\newcommand{\postopt}[0]{post-search}
\newcommand{\Preopt}[0]{Pre-search}
\newcommand{\Postopt}[0]{Post-search}
\newcommand{\absim}[0]{\textit{absenceImpact}}
\newcommand{\overallThreshold}[0]{\textit{overallThreshold}}
\newcommand{\nonterm}[0]{non-terminating}
\newcommand{\unimp}[0]{unimproved}



\newcommand{\preoptElim}[0]{XXX}
\newcommand{\preoptFewerBangs}[0]{YYY}
\newcommand{\preoptPerformance}[0]{ZZZ}
\newcommand{\AoPerformance}[0]{WWW}

\begin{document}
\acmJournal{PACMPL}
\title{\At:\break Minimizing Bangs while Maintaining Performance}
\acmConference[Haskell'18]{11th ACM SIGPLAN International Haskell Symposium}{September 27-28, 2018}{St.Louis, MO, United States}
\author{Marilyn Sun}
\affiliation{Tufts University}
\email{marilyn.sun@tufts.edu}
\author{Kathleen Fisher}
\affiliation{Tufts University}
\email{kfisher@cs.tufts.edu}
\acmYear{2018}
\begin{abstract}

While lazy evaluation has many advantages, it can result in serious
performance costs. To alleviate this problem, Haskell allows users to
force eager evaluation at certain program points by inserting
strictness annotations, known and written as bangs (!).
Unfortunately, manual bang placement
is labor intensive and difficult to reason about. The \Ao{}
optimizer uses a genetic algorithm to automatically infer bang patterns
that improve runtime performance. However, \Ao{} often generates
large numbers of superfluous bangs when given a program to optimize,
which is problematic because users must inspect each such bang to
determine that it does not introduce non-termination or other semantic
differences. 
This paper introduces \At, which uses GHC
profiling information to reduce the number of superfluous bangs.
Specifically,
\At{} adds a \preopt{} phase before \Ao's genetic algorithm
to focus the search space, and a \postopt{} phase to
individually test and remove bangs that have minimal impact. When tested on the NoFib benchmark, the \preopt{}
phase on average eliminated 63~locations for potential bang
placement per 100~LOC and reduced the number of bangs eventually
generated by 5~bangs per 100~LOC. Overall, \At{} reduced the number of
bangs generated from 11 to 1~bangs per 100~LOC, while only
slowing program runtime by 15.7\%. \At{} also eliminated 81.8\% bangs with 15.7\% runtime slowdown when tested on the larger \texttt{gcSimulator} program, 
and eliminated 91.3\% bangs with almost no runtime slowdown on
the \texttt{Aeson} parser library.
\ksf{Check that the statistics on this summary are correct.}
\end{abstract}


\maketitle

\section{Introduction}

\subsection{Lazy Evaluation}
Haskell's lazy evaluation strategy supports modularity, can improve
program efficiency, and enables the use of infinite data
structures~\cite{Hughes89}. Under lazy evaluation, expressions are
only evaluated when their values are needed. Every unevaluated
expression is stored in a \textit{thunk}, and its evaluation is
delayed until another expression demands the value of the current
one~\cite{PeytonJones89}. 

While lazy evaluation reaps many benefits, it can also create serious
performance degradation in both time and space when large amounts of
memory are allocated to thunks~\cite{Jones94,Santos98,Ennals03}.
Reducing the number of thunks created at runtime is an important
optimization in the GHC compiler, which uses a backward static
analysis~\cite{Sergey14} to statically find expressions that the
compiler can safely evaluate immediately rather than converting into
thunks.  Because this analysis is part of the GHC compiler, it must be
conservative, eliminating only thunks that it can prove will not
affect the program semantics when given any possible input.
Unfortunately, this conservative analysis is not always sufficient, in
which case programmers can manually insert strictness annotations such
as \textit{bang patterns}~\cite{bang}, which instruct the compiler to
immediately evaluate the corresponding expression regardless of
whether the strictness analysis determines it is safe to do so.  These
manual strictness annotations can significantly improve
performance~\cite[Chapter~25]{rwh}. However, programmers need to
distinguish program points that will benefit from eager evaluation
from program points that do not need to be evaluated or will not
terminate when evaluated. This task is difficult and often reserved
for expert Haskell programmers~\cite{Mitchell13}.

\subsection{\Ao}

\Ao~\cite{autobahn-wang} is a Haskell optimizer that allows programmers to
reduce thunk usage by automatically inferring strictness
annotations. Users provide \Ao{} with an unoptimized
program, representative input, and an optional configuration file to
obtain an optimized version of the program over the course of a couple
of hours.
\Ao{} uses a genetic algorithm to randomly search for
beneficial locations to place bangs in the program. The genetic
algorithm iteratively measures the performances of a series of
candidate bang placements. Candidates that improve upon the original
program's performance are preserved, and candidates that trigger
non-termination or worsen program performance are
eliminated. \Ao{} eventually returns a list
of well-performing candidates, ranked by how much each candidate
improves program performance. Users can then inspect the candidate
bang placements, and decide if they want to apply one of them to the
program.

\subsection{Too Many Bangs}

Users need to inspect candidate bang placements before applying them
to ensure they maintain the desired semantics on all relevant inputs.
Because \Ao{} measures performance when the program is run on the
supplied input, the resulting annotations optimize the program
\textit{for that specific input}, which is why it is important the
user provides representative input.  When run on different input, the
annotations could change the termintion behavior of the program, which
may or may not be a problem.  We can run GHC's static analysis after
\Ao{} to check the safety of the inferred bangs; on average it
only marks 10\% of them as safe.  The remaining 90\% are either unsafe
on some inputs or their safety is unknown because of the
conservativity of the analysis.


In reality, bangs only need to be safe on inputs that the user will
actually pass to the program.  By analogy, we do not generally mind
that the usual factorial function diverges when passed a negative
number because we know the function is only intended to be applied to
postive numbers.  Because only the user knows the range of valid
inputs, only the user can inspect candidate bang placements to decide
if they are safe on all relevant inputs. However, users face a
time-consuming task when \Ao{} generates many bangs in a candidate
placement.  This task is particularly frustrating when some of the
bangs may not contribute much to program performance. On average,
\Ao{} generates 11~bangs per 100~lines of code in its best performing
candidates, and the user must manually inspect every one of those.

\subsection{\At{}}

This paper presents \At{}, an improved version of \Ao{} that aims
to reduce the number of generated bangs. 
\At{} introduces a \textit{\preopt{}} phase and
\textit{\postopt{}} phase that run before and after \Ao{},
respectively.  Both phases use information from GHC's profiler to
locate and eliminate ineffective bangs. 
GHC profiles are helpful in this regard because they
show the amount of runtime and memory each location in the
program is responsible for. The \preopt{} phase uses this information to adjust the set of files that
\Ao{} considers during its optimization.
Specifically, this phase instructs \Ao{} to
optimize files that contain locations that require significant
resources, potentially adding files not originally mentioned by the
user,  and to skip files that do not. After \Ao{} runs, the \postopt{} phase individually tests
each candidate bang that falls within a costly location. Bangs that
do not significantly impact program performance are
eliminated. On average, the addition of these two phases allows \At{}
to recommend 90.9\%~fewer bangs than \Ao{}, while maintaining similar
runtime improvements. The contribution of this paper are the
following:
\ksf{Check that the statistics on this summary are correct.}
\begin{itemize}
  \item We show how to use profiling information to automatically reduce the number
    of bangs inferred by \Ao{} while maintaining roughly the same
    performance. 
  \item We demonstrate the effectiveness of the \preopt{} phase on
    programs from the NoFib benchmark suite that have had at least one
    file removed from consideration for optimization. There are 20
    such programs in total, which account for 33.3\% of the entire NoFib benchmark suite that we 
    tested on. On average the \preopt{} phase eliminated
    63~potential bang locations per 100~LOC, resulting in a mean bang
    reduction of 63.38\%. \ksf{Does this mean that the preopt phase
      had no effect on the other X programs in the benchmark suite?
      We need to show how often the preopt phase is relevant.}
  \item We use a microbenchmark to show that the \preopt{} phase's
    suggestions for additional 
    files to consider can improve \Ao{}'s optimization results by
    86.6\%. 
  \item We test the \postopt{} phase on the NoFib benchmark suite to show that it can reduce
  the number of generated bangs by 93.8\% at the cost of increasing optimization
  runtime by 33\%.
  \item We show that \At{} applied to the NoFib benchmark suite reduced
    the number of generated bangs by 93\% on average, while only increasing the optimized
    runtime by 15.7\%.
  \item We use \At{} in a case study to optimize the performance of 
    \texttt{gcSimulator}~\cite{Ricci13}, a garbage collector simulator. While the
    program slowed down by 15\%, the number of bangs generated
    decreased by 81.9\%.
  \item We apply \At{} in a second case study to show that it can
    preserve the application-specific annotations inferred by \Ao{}
    for two different uses of the Aeson~\cite{aeson} library.
\end{itemize}


\section{Background}

\subsection{GHC Profiling and Cost Centers}
To focus the search on bangs that are likely to impact performance, we
make use of the information provided by GHC's profiling system.
This system allows users to better understand which locations
in a program consume the most resources.  The system adds annotations
to the program and generates a report detailing the amount of time,
memory allocations, or heap usage that is attributable to each location.
To generate these profiles, users simply run their program on
representative input after re-compiling it with options to requst profiling.
Users can choose to generate either a time and allocation or a heap
profile, as well as the method by which the profiling system adds
annotations. Users can manually specify
annotations or invoke the profiler with  the \texttt{-prof -fprof-auto} option, which
automatically adds an annotation around every binding in the program
that is not marked \texttt{INLINE}.  In the generated profile, each
annotation gives rise to a \textit{cost center} that indicates how
much time or memory were attributable to the given program point as a
percentage of the whole program's resource utilization.  


\subsection{Genes and Chromosomes}

Cost center profiling provides guidance for the otherwise random
search that \Ao{} performs using a genetic algorithm. In the
algorithm, any program source location where a bang may be added is
represented as a \textit{gene} that can be turned \textit{on},
corresponding to the presence of a bang, or \textit{off},
corresponding to its absence.  Since it doesn't make sense to put two
bangs in one program location, there are a fixed number of possible
bang locations in a given program. A \textit{chromosome} comprises all
of the genes within a file. We represent a chromosome as a
fixed-length bit vector, in which each bit indicates the presence or
absence of a bang at the corresponding location. Since a program is a
collection of source files, it is represented as a collection of bit
vectors, or chromosomes.

\subsection{\Ao{}'s Genetic Algorithm}

\Ao{}'s genetic algorithm evaluates and manipulates randomly generated
chromosomes. It repeatedly generates new chromosomes before measuring
their performance using a fitness function. We call a chromosome that
either significantly slows down program performance or causes non
termination an \textit{\unfit{}} chromosome. If the fitness function
determines that a chromosome is \unfit{}, the chromosome is
immediately discarded. If the fitness function determines that a
chromosome behaved well, the chromosome is deemed \textit{\fit{}} and
kept for future rounds of generation.

For each round of chromosome generation, \Ao{} introduces randomness
by performing either a mutation or a crossover. A mutation flips a
gene in the chromosome whenever a randomly chosen floating point
number exceeds the \textit{mutateProb} threshold. A crossover combines
two chromosomes by randomly picking half of the genes from each parent
chromosome. For either of these random operations, \Ao{} uses a unique
number generator each time to guarantee randomness.

\subsection{Optimization Coverage}
\Ao{} uses program source files as the unit of granularity for 
the set of program locations to consider when inferring bangs.  By
default, \Ao{} optimizes all source code files in the directory
containing the program. However, users can specify a different set of
files by manually adding or removing file paths in the \Ao{}
configuration file.  We call the set of files considered in a given
run of \Ao{} its \textit{optimization coverage}.
\Ao{} does not infer bangs for external libraries
that are imported by the program, but users can manually add local
copies of the source code for the external libraries to include them in the optimization process.


\subsection{Representative Input}
To run \Ao{}, users need to provide input on which to run their
program. The input should be short enough for \Ao{} to finish
execution in a reasonable amount of time while still be long enough
for it to measure noticeable time improvements if there are
any.  The input should also be representative of the data the users
intend to supply to the finished program so the system optimizes the
program for that kind of data.  It is this ability to optimize for
relevant input and not all input that gives bangs, whether added by
Autobahn or by the user, the ability to outperform GHC's optimizer. 



\section{\At{}}
\subsection{Why Too Many Bangs Are Generated}

The first step towards reducing the number of inferred bangs is to identify
categories of bangs and to hypothesize why each such category exists.
To that end, we introduce three categories of bangs:
a \textit{\dangerous{}} bang is one that can significantly
degrade program performance, including causing non-termination;
a \textit{\useless{}} bang neither improves nor degrades 
performance but is undesirable because programmers must waste
valuable time reasoning about its safety; finally,
a \textit{\useful{}} bang improves program performance.


An \unfit{} chromosome performs poorly as a whole, but it can contain
a mixture of \dangerous, \useless{}, and \useful{} bangs (By
definition, it must contain at least one \dangerous{} bang).
\Ao{} handles \unfit{} chromosomes by discarding them entirely.  It does not
attempt to identify the \dangerous{} bangs. Removing such bangs from
consideration, rather than simply discarding their chromosomes, would
be useful because otherwise they can reappear in later generations as
the result of a random mutation.

Fit chromosomes face a similar issue. \Ao{} handles \fit{} chromosomes
by preserving the entire chromosome, without separating the \useful{}
bangs from the \useless{} ones. This lack of discrimination is
problematic for two reasons. First, we might lose \useful{} bangs in
future generations because we cannot guarantee that they will be
preserved by the mutation and crossover operations of the genetic
algorithm.  Second, \useless{} bangs can survive if they are grouped
with  \useful{} ones. The accumulation of such \useless{} bangs
can dramatically increase the number of inferred bangs, leaving the
user with a substantial reasoning task.  As program source code
increases in size, the number of \useless{} bang positions also grows,
increasing the likelihood the genetic algorithm will infer and
preserve \useless{} bangs that happen to live on a chromosome
with \useful{} ones. 

\cut{
We hypothesize that \Ao{} generates large numbers of superfluous bangs
because it does not identify the different categores of bangs within
the same chromosome. 
The randomness of \Ao{}'s genetic algorithm means the search space is
never reduced in size.

The further addition of randomness means that the entire
chromosome is repeatedly searched as the search space is never
definitively reduced. Because there is a fixed number of genes in a
program, the search space for the genetic algorithm is also
equivalently fixed. Therefore, as the program source code increases in
size, the algorithm also generates significantly more bangs as
chromosomes increase in size.
}

\subsection{The Solution}

Precisely identifying which bangs are \useless{}/\useful{} is undecideable, but
we can use GHC profiling to make an educated guess. 
Intuitively, a bang
that appears in a cost center that uses many resources, which we 
call a \textit{\hotspot}, is more likely to be \useful{}, while one in a
cost center using few resources, which we call a \textit{\coldspot}, is likely to
be \useless{}.  Leveraging that intuition, \At{} preserves bangs that appear
in \hotspots{} while eliminating those in \coldspots.

We introduce the \hotspotcost{} parameter to determine
which cost centers to consider hot:  a cost center consuming more
than \hotspotcost{} resources is considered hot, while
one consuming fewer is cold.  
Currently, we set \hotspotcost{} 
to 6\% of the overall program runtime, although that threshold can
be adjusted. As the threshold increases, \At{} preserves fewer bangs
at the risk of greater degradations in program performance.


\cut{
isolate portions
of a chromosome by their individual contributions to program
performance. Cost centers not only break down a chromosome into
smaller portions by source code bindings, but their associated costs
also imply how likely a bang placement will affect program
performance. If executing code at a \hotspot{} occupied a significant
portion of the overall program runtime, then a bang-induced change in
performance at the \hotspot{} will likely significantly affect overall
runtime as well. }

\At{} uses GHC profiling information to reduce the
number of generated bangs in two different ways, corresponding to two
different phases in the optimization pipeline, which is shown in \figref{fig:pipeline}.  The first phase, which
we call
the \Preopt{} phase, eliminates
any chromosome corresponding to a program source code file that
contains only \coldspots{}. \At{} invokes \Ao{} only on chromosomes
corresponding to files that contain at least one \hotspot{}.  Removing
chromosomes that are unlikely to contain \useful{} bangs reduces the
size of the search space with little chance of inadvertently
eliminating good sets of annotations.  The smaller search space is
likely to make \Ao{} more effective in improving performance.  In
addition, the system will not infer bangs for any of the genes on the
eliminated chromosomes.  Since such bangs are very likely to have
been \useless{}, this step is very likely to reduce the number of
inferred \useless{} bangs.

The second phase, which we call the \Postopt{} phase, uses profiling
information is to categorize as likely \useless{}/\useful{} bangs that occur within \hotspots{} on
chromosomes that \Ao{} determines to be fit.  In a world with infinite resources,
we would test all combininations of such bangs and select the best one
because the effect of combining bangs is hard to predict.
Unfortunately, the exponential search space is too large to explore
exhaustively. Instead, we individually turn off one such bang at a
time, measure the resulting performance, and keep only those that
significantly impact performance.  The number of such bangs is
sufficiently small that we can test each in turn because the number of
bangs in \hotspots{} in a program is relatively small.

In the rest of this section, we exlain \At{}'s optimization pipeline
in more detail.

\begin{figure}
\tikzset{%
  >={Latex[width=2mm,length=2mm]},
            base/.style = {rectangle, rounded corners, draw=black,
                           minimum width=4cm, minimum height=1cm,
                           text centered, font=\sffamily},
  activityStarts/.style = {base, fill=blue!30},
       startstop/.style = {base, text width=4cm, fill=red!30},
    activityRuns/.style = {base, fill=green!30},
         process/.style = {base, text width=3cm, fill=orange!15,
                           font=\ttfamily},
}

\begin{tikzpicture}[node distance=1.5cm,
    every node/.style={fill=white, font=\sffamily}, align=center]
  \node (preo)             [activityStarts]              {\Preopt{}};
  \node (user)     [process, left of=preo, xshift=-3cm]          {Original Program, Representative Input, Autobahn Configuration};
  \node (autobahn)      [activityStarts, below of=preo, yshift=-2.5cm]
                                                      {\Ao{}};
  \node (end)      [startstop, left of=autobahn, xshift=-3cm, yshift=1cm]
                                                       {If program is unsuitable for optimization: Alert user and end process};
                                                  
  \node (posto) [activityStarts, below of=autobahn, yshift=-2cm]
                                                    {\Postopt{}};     
  \node (endmin)      [startstop, below of=end, yshift=-1cm]
                                                       {If negligible optimization improvement: Alert user and end process};                                                       
   \node (result)     [process, left of=posto, xshift=-3cm, yshift=-1.5cm]          {Optimized and minimized program};                                                   

  \draw[->]             (preo) -- node[text width=4cm]
  					{Remove \coldspots{}, suggest external libraries, check if program is unsuitable for optimization}(autobahn);
  \draw[->]     (user) -- (preo);
  \draw[->]      (autobahn) -- node[text width=4cm]
  					{Find winning chromosome using genetic algorithms}(posto);
   \draw[->]      (preo) -- (end);
   \draw[->]      (posto) -- (endmin);
  \draw[->]      (posto) -- node[text width=4cm, xshift=-2cm, yshift=0.8cm] {Individually test each bang in winning chromosome}
                                   (result);
  \end{tikzpicture}
\caption{\At{} optimization pipeline.}
\label{fig:pipeline}
\end{figure}


\subsection{\Preopt{} Profiling}

Just as manually reasoning about bang placement can be difficult, 
so too reasoning about which files to optimize can be challenging.
\At{} makes this process easier for the user by leveraging information
in GHC profiles.  
To that end, the \preopt{} phase begins by generating a GHC time
and allocation profile for the unoptimized program by running it on 
user-provided representative input.
\At{} then sets the optimization coverage
for \Ao{} to be source files that contain at least one \hotspot{}. 
In addition, 
\At{} identifies library files that contain \hotspots{} and
suggests to users that they add local copies so the library source
files may be optimized as well. 
\Ao{} then optimizes the program as usual, except using a more
targeted set of files/chromosomes.
Note that this phase can both expand the search space, by identifying
library files that contain hotspots, as well as reduce it, by
identifying program source files with no \hotspots{}. 

\Preopt{} profiling offers three important benefits.
First, it can greatly reduce the number of bangs \Ao{} suggests by
limiting the search space to those program files that have a chance of
significantly impacting performance. This focusing reduces the
possibility of generating \useless{} or \dangerous{} bangs by
eliminating them from consideration
and
increases the chances of generating \useful{} ones by allowing more of
the relevant search space to be explored within a given time budget.
Second, if a \hotspot{} is located in an external library file,
the \preopt{} phase can identify the relevant files so they can be
included in the optimization process. 
Third, it identifies programs that are potentially unsuitable for
\Ao{} optimization. If a program contains a large number of cost
centers that all contribute minimally to program runtime, there may
not be any way to substantially improve program performance by adding
bang annotations. If the \preopt{} phase concludes that
a program only contains \coldspots{}, it will alert the user and
abort, saving the time and effort of running the remaining phases, 
which are unlikely to identify significant performance improvements.


It is worth noting that although the \preopt{} phase can reduce the
size of the search space, it does not reduce \Ao{}'s runtime as a
result. Unless the search space is small enough to be exhaustively
searched, \Ao{} will search until consuming its entire time budget:
if a user allowed \Ao{} to run for two hours, it
will run for two hours regardless of the search space
size. The reduced search space does allow \Ao{} to explore the space
more thoroughly, statistically speaking, potentially allowing it to 
find better results within the fixed time budget.


\subsection{\Postopt{} Bang Elimination}

After \Ao{} explores the search space and suggests a winning set of
chromosomes, \At{} uses GHC profiling information to eliminate any
bangs on those chromosomes that don't significantly contribute to
program performance. To that end, 
the \postopt{} phase decides that any gene that is off in the winning
chromosomes should not have a bang in the final recommendation.
It removes these genes from further consideration. 
The \postopt{} phase then maps each remaining gene, which must be
turned on, to its corresponding cost center.  It turns off all genes
that do not fall within \hotspots{}, deciding not to recommend bangs
for the corresponding locations on the theory that such bangs are
likely to be \useless{}.  It then removes these genes from further
consideration. 

The remaining genes are the interesting ones in that they both
contain a bang and fall within a \hotspot{}. These genes require
further testing because they may still be \useless{}, failing to
improve progam performance even though they fall within a \hotspot{}.
If they are \useless{}, they should be discarded. 
There is usually a sufficiently small number of remaining genes that
are both turned on and within a \hotspot{} that it is possible to
measure their impact one by one.  Specifically, 
the \postopt{} phase tests each such gene in turn, turning
it off while keeping all the remaining bangs on.  It 
then runs the resulting program and compares its performance to that
of the winning chromosomes as determined by \Ao{}.
If the absence of the bang slows down the program by the value of the \absim{}
threshold parameter, the \postopt{} phase deems that the bang is useful and
decides to keep it. 
If the bang's absence does not slow down the program by at
least the \absim{} threshold, the bang is deemed \useless{} and
discarded. The \absim{} threshold is adjustable and currently set to
5\%. The \postopt{} phase repeats this process for every bang
that is on in the winning chromosomes and in a \hotspot{}. 
The  minimization result is the combination of all the surviving
bangs at the end of testing.

\subsection{Representative Input}
Just as with \Ao{}, the quality of the representative input impacts the quality of
\At{}'s performance because different inputs may
generate wildly different results in GHC profiles. Therefore, users
must carefully choose the representative input.


\section{Implementation}

\subsection{Program Architecture}
\At{} wraps \Ao{} with the  \preopt{} and \postopt{} phases as shown
in \figref{fig:pipeline}.  

Prior to running the genetic algorithm, the \preopt{} phase invokes
GHC's profiler on the unoptimized version of the user's program with
the \texttt{-prof -fprof-auto} option to auto-generate cost centers
around every non-inlined program binding.  It uses the supplied
representative data as input to the program to generate the time and
allocation profile.  This baseline profile is only generated once, and
the rest of \At{} refers to the same profile for \hotspot{}
information throughout the entire optimization pipeline.
Depending on the where the \hotspots{} fall according to the profiler, 
the program's optimization coverage will either be
automatically reduced or manually expanded by the user.

Then \At{} reuses the existing implementation
of \Ao{}~\cite{autobahn-wang} to identify a winning set of 
chromosomes.
\Ao{} uses the \textit{haskell-src-exts}~\cite{langexts} parser to parse
source files and to identify genes.  
It then applies a genetic algorithm,
implemented using the \texttt{GA} Haskell library~\cite{genetic}
with a fitness function based on measured runtime performance, to search for the best performing chromosomes.

The \postopt{} phase eliminates bangs from the best-performing
chromosomes returned from \Ao{}.  It does so by re-running the program
once for each gene that both lies in a \hotspot{} and is turned on in
the winning chromosomes.  Specifically, it turns off the gene, runs
the program on the representative input, measures its performance, and
keeps any bang that impacts performance more than the \absim{}
threshold.  Similar to \Ao{}, the postopt{} phase uses
the \textit{haskell-src-exts} library to parse the relevant source
files, set all the bangs appropriately, and then pretty-print the
modified source code to then be compiled by GHC and run on the
representative input. 

Note that this process does not require re-running the original
profile.  Instead, we reuse the profiling information calculated
during the \preopt{} phase.  Bangs in \hotspots{} are tested in order
of decreasing costs. While we recognize that the order in which we
test the bangs may affect the performance, it is too time consuming to
test the bangs in all possible orders.  For simplicity, we chose to
test each bang once in order of decreasing costs.

\ksf{For your thesis or for the final version, it might be worth
playing around a bit to see if the order makes any difference.  For
example, do the results change substantially if you wonder them in
increasing order instead?   Or is it just random.}

Finally, the \postopt{} phase returns the final combination of bangs
that have survived each round of testing. If \Ao{} failed to find 
chromosomes that improved program runtime by at least the
\overallThreshold{} parameter, then
the \postopt{} phase will return the unoptimized program.
We base this choice on the
theory that the relatively insignificant performance improvement
indicates users are better off keeping the original program rather
than having to reason about the safety of the inferred bangs. 
Currently, we set the \overallThreshold{} value to 6\%, but users
may change it. 



\cut{
The resulting chromosome is further tested and reduced using GHC
profiling information in the \postopt{} bang reduction phase. After an
initial pass of elimination to get rid of all turned-off bangs and
bangs located in \coldspots{}, we individually test the impact of the
absence of a turned-on bang in each \hotspot{}. If a bang meets
the \absim{} threshold, it is kept in future rounds of testing and
will remain in the final combination of bangs for the optimized
program. If a bang does not meet the \absim{} threshold, it is removed
for future rounds of testing and will not appear in the final
combination.
}


\subsection{Running \At{}}

Users run \At{} the same way as they would run \Ao{}. They
provide a copy of their program source code, representative input,
and an optional configuration file. The \preopt{}
and \postopt{} phases typically require a negligible amount of time
compared with the time required to run \Ao{}.

If \At{} completes successfully, users can find the resulting source
code with the minimized bang annountations in the same project
directory as they would find the usual \Ao{} \texttt{survivor}
and \texttt{results} directories. If \preopt{} profiling detected that
the program is unsuitable for optimization, or if \Ao{} failed to
significantly optimize the program, then \At{} warns the user and
halts the optimization process. If external libraries could be added to
the optimization coverage to potentially improve the results,
the \preopt{} phase alerts the user and then continues to optimize as
normal.

\subsection{Source Locations}
\Ao{} uses a bit vector to represent the genes in a
chromosome, with each bit corresponding to a possible bang location in
a source file.  In \At{}, we modified this representation so that each
bit also indicates the cost center associated with the possible bang
location.   Cost centers are uniquely identified by the corresponding source
locations in source files.  Thus, we mapped each bit to its corresponding
source line. To turn the bangs in a \hotspot{} on or off, we can
traverse the bit-location vector and manipulate the bits that are
tagged with source lines that fall within the range of
that \hotspot{}'s source location.

\subsection{Removing Illegal Genes}
Both \Ao{} and \At{} use the \textit{haskell-src-exts} parser to add
and remove bangs.  
Unfortunately, the version of \textit{haskell-src-exts} parser that we use
incorrectly identifies the left-hand side of bindings within instance
declarations as potential locations to place bangs. For that reason,
\Ao{} did not optimize files that contain instance declarations. 
To eliminate this restriction, we instead removed any randomly
generated illegal bangs prior to each round of fitness evaluation in
the genetic algorithm. The rest of the genetic search runs
identically as before.

To keep track of whether a bang is legal, we used a
validity-indicating boolean vector to represent whether each gene in
the source code is legal. Prior to inserting bangs into a
program, \At{} would check the validity of a gene against the boolean
vector to make sure that the bang is located in a legal location.

Generically traversing the parser-generated AST using boilerplate code
fails to identify illegal genes, so we needed to manually traverse the AST
to construct the validity vector. As we traversed the AST, we kept
track of whether a left-hand side binding is within an instance
declaration. If so, then that binding is an illegal bang location and
is marked as \texttt{false} in the validity vector. All other
legal bang locations are marked as  \texttt{true}.

With this approach, \At{} successfully avoids inserting bangs into
illegal locations.  As a result, \At{} can now optimize fiels that
include instance declarations, which was not previously possible.

\section{Evaluation}

\subsection{Experimental Setup}

All versions of Autobahn were compiled using GHC
version \texttt{8.0.2}. The NoFib benchmarks were compiled with
GHC version \texttt{8.0.2} using NoFib's default flags, the flag
\texttt{-XBangPatterns}, and the NoFib flag that enables profiling. 
We discarded the benchmarks in the NoFib suite that failed to compile
or run out of the box. We also excluded the benchmarks that \Ao{}
refuses to optimize because they already have very fast runtimes. That
left 60 benchmarks on which we could run experiments.

To study runtime performance results, we report what we call
the \textit{runtime performance ratio}, which is the ratio of the
optimized program's runtime to the runtime of the original program.
Ratios less than 1 are considered successful because they indicate the
optimized program performs better than the original, while higher
ratios are failures.  A runtime performance ratio of 1 indicates that
the optimized version of the program has the same runtime as the
original version and is thefore \textit{\unimp{}}.  A runtime
performance ratio of 2 is a sentinel value indicating that the
``optimized'' version of the program is \textit{\nonterm{}}, by which
we mean it runs for more than a timeout threshold longer than the
unoptimized program.  Failures have the effect of leaving the program
unchanged as the proposed annotations are discarded in favor of the
original.

To calculate the performance of a particular benchmark using a
particular optimization system (\eg{}, \At{} or a 
restricted version using only a subset \At's phases), we ran the system under test on the benchmark 10
times to account for random fluctations.
We scored each run by its runtime performance 
ratio. Note that each run of the benchmark can produce a different
result not only because of variations in machine load but also
because the search process is randomized.  Failed runs can be further
categorized into runs in which \Ao{} fails and runs in which
the \postopt{} phase of \At{}
fails. If a benchmark failed at the \Ao{} stage, then we say that its
runtime performance ratio for that run is 1 and it recommended
0~bangs. If a benchmark succeeded at the \Ao{} stage but failed
in the \postopt{} phase, then we say its runtime performance ratio is 1 and the
number of inferred bangs is the number of bangs suggested by the \Ao{}
phase. Finally, we computed the \ksf{harmonic} mean of the runtime
performance ratios and the \ksf{arithmetic} mean of the number of
recommended bangs.    

When we study the results of a particular optimization system on a
particular benchmark, we also categorize the results into one of three
groups: complete success, partial success, and complete failure.  A
benchmark completely succeeds if all of its runs are succesful,
completely fails if all of its runs are failures, and partially
succeeds if there is a mixture of these outcomes.

Finally, to compute summary statistics across all the benchmarks, 
we calculate the \ksf{harmonic} mean of the runtime performance ratios
and the \ksf{arithmetic} mean of the total number of suggested bangs
across all 60 benchmark programs. 


\subsection{\Preopt{} Search Space Reduction}

To assess the impact of the \preopt{} phase, we study the number of
genes that the phase eliminated (or added) for consideration by \Ao{}.
We compare the number of bangs that \Ao{} inferred when run with and
without the \preopt{} phase, calling the optimizer comprised of
the \preopt{} phase plus \Ao{} the \textit{\Preopt{} optimizer. } We
note in how many of the runs the \Preopt{} optimizer failed.  Finally,
we compare the runtime performance ratio for \Ao{} with the ratio for
the \Preopt{} optimizer.
To compute this data, we ran both the \Preopt{} optimizer and \Ao{} on
the 60 programs from NoFib benchmark suite.  We optimized all runs 
on runtime only, and we set both the \hotspotcost{} and \absim{} thresholds 
to 6\%.

The \preopt{} phase eliminated at least one file in 20 out of the 60
benchmark programs. 
The remaining 40 benchmarks fall into one of three categories.
The first category consists of benchmarks that only had one file to
begin with. The second contains benchmarks that did not have any files
eliminatd by the \preopt{} phase.  The third category comprises
benchmarks that the \preopt{} phase determined were not suitable for
optimization because they had no significant \hotspots{}. 
Since \Ao{} and the \Preopt{} optimizer handle these 40 benchmarks
identically, we exclude them from the graphs reporting on the results
of the \preopt{} phase. We discuss the third category of benchmarks in 
\secref{sec:file-elim}. \ksf{How many programs were in each
category?}  \ksf{Is this right:} None of the 60 programs had an
external \hotspot{} not in the original optimization coverage.

\figref{fig:preopt-bangs} shows the results from the 20 benchmarks that
had at least one file eliminated during the \preopt{} phase. The
\texttt{Eliminated Genes} column gives the number of potential bang
locations that were eliminated before \Ao{} ran. 
The \texttt{Original Bangs} column records the number of bangs in the
original program, most of which were 0. 
The \texttt{Failed runs} column gives the fraction of the 10 runs on
which the \Preopt{} optimizer failed. 
Finally, the \texttt{\Ao{} Bangs} and \texttt{\Preopt{} Bangs} columns
give the number of recommended bangs by the two systems,
respectively.  On average, the \preopt{} phase eliminated
\preoptElim{} bangs across the 60 programs in the benchmark suite before
invoking \Ao{} and the \Preopt{} optimizer recommended \preoptFewerBangs{} fewer
bangs. 

The \texttt{anna}, \texttt{expert}, and \texttt{symalg} benchmarks are
particularly interesting because \Ao{} consistently failed to find
winning chromosomes for them, and so it did not generate any bangs. However,
after reducing the size of the search space, the \Preopt{}
optimizer was able to find meaningful bangs.

\figref{fig:preopt-runtime} shows the corresponding runtime
performance ratios produced by \Ao{} and the \Preopt{} optimizer on
the same 20 benchmarks.
The graph shows that even when a large number of genes are
eliminated prior to running the genetic algorithm, the optimizer is still able to
find \useful{} bangs that result in similar runtime improvement. This data
confirms that the eliminated genes were on the whole not \useful{}.
On average, the \Preopt{} optimizer returns runtime performance ratios
of \preoptPerformance{} compared to \AoPerformance{} for \Ao.

\begin{figure*}
\includegraphics[width=\textwidth]{pre-aut-bangs}
\caption{Number of bangs generated by \Ao{} vs. \Preopt{} optimizer across 20 benchmarks that had at least
one file eliminated during the \preopt{} phase. Columns that exceed
the maximum axis value are labeled with their actual values. The
benchmarks are sorted in increasing order of number of failed runs for
the \Preopt{} optimizer.}
\label{fig:preopt-bangs}
\end{figure*}

\begin{figure*}
\includegraphics[width=\textwidth]{pre-aut}
\caption{Peformance runtime ratios of \Ao{} vs. \Preopt{} optimizer
across 20 benchmarks that had at least one
file eliminated during the \preopt{} phase. The x-axis 
is on a base-2 log scale. The benchmarks appear in the same order as
in \figref{fig:preopt-bangs}.}
\label{fig:preopt-runtime}
\end{figure*}

\subsection{\Preopt{} File Elimination}
\label{sec:file-elim}

The preopt{} phase identifies 6 benchmarks among the 60 in our suite
as unsuitable for optimization when the \hotspotcost{}
threshold is set to 6\%:  \texttt{awards}, \texttt{callback001},
\texttt{callback002}, \texttt{mutstore2}, \texttt{sorting}, and \texttt{threads007}. As
expected, when attempting to
optimize \texttt{awards}, \texttt{sorting},
and \texttt{threads007}, \Ao{} consistently fails, returning
the \unimp{} runtime code. However, \Ao{} was able to
successfully optimize the other programs. Through inspection, we
concluded that \texttt{callback002} would have benefited from a
lower \hotspotcost{} threshold as its most costly hot spot takes up
3.9\% of the program runtime. Both \texttt{callback001}
and \texttt{threads007} would have benefited from inspecting heap
profiles instead of time and allocation profiles as the costs
associated with their hot spots were noticeably larger in heap
allocations while remaining insignificant in runtime
costs. The \texttt{mutstore2} program's performance fluctuated wildly
even without bangs in it. For example, its measured runtime was as low
as 60\% - 80\% of its original runtime in one-third of the experiments
we ran with no bangs in the program. Therefore, the optimization
results were likely skewed by the fluctuating runtime.

\subsection{\Preopt{} File Addition}
\ksf{We could cut this section if we need to save a lot of space.}
To demonstrate the effectiveness of using the \preopt{} phase to
expand Autobahn's coverage for improved optimization results, we
tested our approach on the \texttt{sumList} microbenchmark. We
created \texttt{sumList} to simulate the scenario in which a
programmer references code from an external library or external file
that contains \hotspots{} but is not within the current optimization
coverage. 

The \texttt{sumList} program's \texttt{Main.hs} file contains only one
function: a main function that constructs a list of integers from 1 to
1,000,000 and then calculates the sum of all integers in the list using an
external \textit{sum} function located in \texttt{Sum.hs}. Users
may decide to set the optimization coverage to [\texttt{Main.hs}],
because they are interested in making the main program run
faster. However, as demonstrated in \tblref{table:sumList}, \Ao{} was only able to
improve program performance by 3\%, even when it was able to
exhaustively search the possible bang locations in the 6 lines of code in the \texttt{main}
function. Upon inspecting the results, users may be mistaken in
believing that their program runtime cannot be improved
further.

But there are indeed other opportunities to speed up \textit{sumList}
located in places that the users did not think about. If the users were
to rerun the optimization using the \preopt{} phase, then GHC's time and
allocation profile indicates that the largest \hotspotcost{} was 9.5\%
and located in lines 7 to 8 in the \textit{sum} function
in \texttt{Sum.hs}. The \textit{sum} function is entirely lazy and did
not immediately compute the sum of each integer as it recursed through
the list. In the resulting log file, the \preopt{} phase 
suggests 
adding \texttt{Sum.hs} to the optimization coverage.
After expanding the coverage and re-running the optimization,
the resulting \texttt{sumList} ran at only 13\% of the original
runtime, a dramatic improvement.

Although the \texttt{sumList} example is short and synthetic, it shows
the larger potential for users to obtain much better optimization
results when running the \preopt{} phase in conjunction
with \Ao{}. Programmers often build upon each other's code and use
external functions that they may not be entirely familiar with or did
not consider optimizing. The \preopt{} phase can identify valuable
missed opportunities. Of
course, the addition of more files to optimize means that more bangs
might be generated. It is up to the user to decide if they want to add
the suggested files for better optimization results at the risk of
needing to inspect more bangs.
\newline
%We also ran experiments using the Aeson parser library that handles JSON files using either the \texttt{validate} or \texttt{convert} driver programs. \texttt{validate} simply checks if the file is written in valid JSON syntax, and \texttt{convert} actually converts file input into a Haskell data structure. 

%While running the \preopt{} phase on the Aeson library, we were suggested by the program to include files from the external \texttt{Data/Aeson} library in the Autobahn optimization coverage because the \texttt{Data/Aeson/InternalTypes.hs} file included multiple \hotspots{}. However, \texttt{InternalTypes.hs} already included manually inserted bangs by its author. Therefore we ran experiments using a version of \texttt{InternalTypes.hs} with no bangs in it to see if we could recreate the manually inserted bangs. If so, library authors could run Autobahn to place bangs in their files instead of manually doing so. 


\begin{table}
\begin{tabular}{p{2.5cm}p{1.5cm}p{2cm}p{1cm}}
\hline
Version   & Coverage & Normalized Runtime & No.Bangs \\
\hline
Original      & N/A   &   1	 & 0   \\
\Ao{}       & [\texttt{Main.hs}]      & 0.97    &  2\\
Pre-optimization	& [\texttt{Main.hs}, \texttt{Sum.hs}]         & 0.13      & 4\\
\hline
\end{tabular}
\caption{Results for \texttt{sumList} microbenchmark.}
\label{table:sumList}
\end{table}
%\begin{tabular}{llr}
%\hline
%Version   & Driver & Normalized Runtime \\
%\hline
%Original      & value   & 1     \\
%          & json        & 1      \\
%Pre-optimization       & value     & 0.73     \\
%          & json        & 0.66	\\
%\Ao{}       & value     & 0.56      \\
%          & json        & 0.72	\\
%\hline
%\end{tabular}

\subsection{\Postopt{} Bang Reduction}

To test the efficiency of reducing bangs using the \postopt{} phase, we compared the results of running only \Ao{} with \Ao{} and the \postopt{} phase. Similarly, we took the mean of running the program ten times on the NoFib benchmark suite while optimizing on runtime only, and set both \hotspotcost{} and \absim{} thresholds to 6\%. 


A benchmark is successfully optimized if \Ao{} improved its performance by at least 6\% after optimization. Figure 3 and Figure 4 include results from the 21 benchmarks that were consistently successfully optimized by \Ao{} in each run. Figure 5 and Figure 6 include results from the 28 benchmarks that did not succeed in every run but succeeded at least once, sorted in increasing order of failure rate. We did not include results from the 12 remaining benchmarks that failed to optimize by \Ao{} in every run. Figure 7 shows how frequently a benchmark failed at the \Ao{} stage and how frequently it failed at the end of the \postopt{} phase.

Figure 3 shows that the number of bangs eliminated by the \postopt{} phase is quite significant. Figure 4 shows the corresponding runtime performance of each optimized program. In most benchmarks, the \postopt{} phase does a little worse than running only \Ao{}, because the \absim{} threshold limits the remaining bangs to those that affect program runtime by at least 6\%. If a user wants to maintain more similar runtime results, they can lower the \absim{} threshold so the minimizer becomes less aggressive in bang elimination. That way, more bangs will be preserved, but runtime performance will improve. The \texttt{callback001} and \texttt{pic} benchmarks are good examples of when a program has no cost centres that meet the \hotspotcost{} threshold, therefore all its bangs were eliminated by the \postopt{} phase. In this scenario, a lower threshold would've helped discover useful bangs that are located in the relatively less costly bangs. 

The interesting results for programs \texttt{anna} and \texttt{fluid} show that while \Ao{} found bangs that triggered a \nonterm{} runtime code, \postopt{} bang elimination was able to get rid of the \dangerous{} bangs that caused non termination and produce successful optimization results. 

\begin{figure*}
\includegraphics[width=\textwidth]{aut-post-bangs}
\caption{Number of bangs generated by \Ao{} vs. \postopt{} phase combined with \Ao{} across 21 benchmarks that successfully optimized every time. Columns that exceed the maximum axis value are labelled with their actual values. }
\end{figure*}

\begin{figure*}
\includegraphics[width=\textwidth]{aut-post}
\caption{Normalized runtime of \Ao{} results vs. \postopt{} phase combined with \Ao{} results across 21 benchmarks that successfully optimized every time. The x-axis normalized runtime is on a log scale of base 2. Columns that exceed the maximum axis value are labelled with their actual values. *\Ao{} resulted in non-terminating optimization results for \texttt{anna} and \texttt{fluid}, and the \postopt{} phase was able to remove the non-terminating bangs to end non program non-termination. }
\end{figure*}

\begin{figure*}
\includegraphics[width=\textwidth]{ap-partial-bangs}
\caption{Number of bangs generated by \Ao{} vs. \postopt{} phase combined with \Ao{} across 27 benchmarks that sometimes succeeded to optimize. Success rate out of 10 runs is shown. Columns that exceed the maximum axis value are labelled with their actual values. Graph is sorted in increasing order of failure rate.}
\end{figure*}

\begin{figure*}
\includegraphics[width=\textwidth]{ap-partial}
\caption{Normalized runtime of \Ao{} results vs. \postopt{} phase combined with \Ao{} results across 27 benchmarks that sometimes succeeded to optimize. The x-axis normalized runtime is on a log scale of base 2. Success rate out of 10 runs is shown. Columns that exceed the maximum axis value are labelled with their actual values.}
\end{figure*}

\begin{figure*}
\includegraphics[width=\textwidth]{aut-post-fail}
\caption{Frequency of \Ao{} failure vs \At{} failure across 27 benchmarks that sometimes succeeded to optimize. Graph is sorted in increasing order of \At{} failure rate. }
\end{figure*}

\subsection{Combining \preopt{} and \postopt{}}

To test for the overall effectiveness of running \At{}, we took the average results of running both the \preopt{} and \postopt{} phase together on the 60 Autobahn benchmarks again. Out of those 60 benchmarks, 5 benchmarks were eliminated during the \preopt{} phase for having no \hotspots{} and 3 benchmarks were consistently unable to be optimized by either \At{} or \Ao{}. The remaining 52 benchmarks succeeded to optimize at least once during the ten runs, and their results are presented in figure 8, 9, 10 and 11. The benchmarks are sorted in increasing order of failure rate and split into two sets of graphs of 26 benchmarks each. Figure 8 and 10 show the results of bang reduction, and figure 9 and 11 show the corresponding runtime performances of each benchmark. Figure 12 shows how frequently a benchmark failed at the \Ao{} stage and how frequently it failed at the end of \At{}.

While most benchmarks consistently showed significant bang reduction with minimally compromised performance, a few benchmarks stand out. The \texttt{expert} and \texttt{calendar} benchmark not only had bangs reduced by 79.41\% and 97.63\% respectively, but also experienced a performance speed up by 27.59\% and 14.82\% respectively. Although it is worth noting that both benchmarks failed more times than they succeeded, so such results are not guaranteed to be replicable in every run. The \texttt{atom} benchmark also has interesting results because the \postopt{} phase eliminated all bangs generated by \Ao{}, yet it still ran at 78\% of it's original runtime. This potentially suggests that \texttt{atom}'s original runtime fluctuates by a significant amount on its own. It also potentially suggests that \texttt{atom}'s overall performance improvement is achieved through the accumulation of speedups at many low-cost cost centres, so lowering \At{}'s \hotspotcost{} would help capture those cost centres with lower associated costs and result in better performance improvement.

Another interesting observation is that there are only 8 benchmarks that consistently failed to optimize in each run, while there were 12 benchmarks that consistently failed when being optimized with only \Ao{} and the \postopt{} phase. Out of the 13 benchmarks that failed in the only \Ao{} and \postopt{} experiments, 4 of them were able to be successfully optimized when they had at least one file eliminated in the \preopt{} phase. This suggest that adding in the \preopt{} phase potentially increased the number of benchmarks that were able to be successfully optimized.

\begin{figure*}
\includegraphics[width=\textwidth]{pap0-bangs}
\caption{Number of bangs generated by \Ao{} vs. \At{} across the first 26 of 52 benchmarks that successfully optimized at least once. Success rate out of 10 runs is shown. Graph is sorted in increasing order of failure rate. Columns that exceed the maximum axis value are labelled with their actual values.}
\end{figure*}

\begin{figure*}
\includegraphics[width=\textwidth]{pap0}
\caption{Normalized runtime of \Ao{} results vs. \At{} results across the first 26 of 52 benchmarks that successfully optimized at least once. The x-axis normalized runtime is on a log scale of base 2. Success rate out of 10 runs is shown. Columns that exceed the maximum axis value are labelled with their actual values.}
\end{figure*}

\begin{figure*}
\includegraphics[width=\textwidth]{pap1-bangs}
\caption{Number of bangs generated by \Ao{} vs. \At{} across the remaining 26 of 52 benchmarks that successfully optimized at least once. Success rate out of 10 runs is shown. Graph is sorted in increasing order of failure rate.Columns that exceed the maximum axis value are labelled with their actual values.}
\end{figure*}

\begin{figure*}
\includegraphics[width=\textwidth]{pap1}
\caption{Normalized runtime of \Ao{} results vs. \At{} results across the remaining 26 of 52 benchmarks that successfully optimized at least once. The x-axis normalized runtime is on a log scale of base 2. Success rate out of 10 runs is shown. Columns that exceed the maximum axis value are labelled with their actual values.}
\end{figure*}

\begin{figure*}
\includegraphics[width=\textwidth]{pap-fail}
\caption{Frequency of \Ao{} failure vs \At{} failure across the 36 benchmarks that sometimes succeeded to optimize. Graph is sorted in increasing order of \At{} failure rate. }
\end{figure*}

\subsection{\At{} On Larger Programs}

We ran \At{} on the gcSimulator garbage collector to see how well it performs on larger programs. To keep optimization runtime within reasonable ranges, we used the first 1M of the batik trace file as the representative input. For gcSimulator, we lowered the \absim{} threshold to 1\% because it does not have many \hotspots{} to begin with. Once \At{} was done optimizing, we tested the resulting program on larger trace file sizes of 100M and 500M. Because running the garbage collector on the full 6184M trace size took too long with or without optimization, we did not record results for the full trace. 

The original \Ao{} was able to produce results that not only ran faster on representative input, but also on larger trace files as well. \At{} was also able to generate similar results with much fewer bangs, as demonstrated in the table below. 

\begin{tabular}{lllr}
\hline
Version   & File Size (M) & Runtime & No.Bangs \\
\hline
Original      & 1   &   0.40	 & 0   \\
          & 100        & 43.13      & 0 \\
       & 500     &  216.71 & 0 \\
\Ao{}       & 1     & 0.18    &  690\\
          & 100        & 14.19 &  690\\
                 & 500        & 68.98	& 690\\
\At{}      & 1   &  0.23 & 125    \\
          & 100        & 15.75 & 125      \\
       & 500    & 81.66 & 125    \\

\hline
\end{tabular}
 
\subsection{\At{} On Aeson}
 

 The original \Ao{} experiments also showed that its genetic algorithm was able to infer application-specific bang patterns for the \texttt{Aeson} program. The \texttt{Aeson} program is a \texttt{JSON} file parser that can be used with one of two drivers that operate differently on the \texttt{JSON} file it parses. The \texttt{validate} driver checks if the file is a valid \texttt{JSON} file without the need to completely evaluate each \texttt{JSON} value. The \texttt{convert} driver converts the \texttt{JSON} file into a Haskell data structure, and thus it needs to completely evaluate each \texttt{JSON} value. If the \texttt{Aeson} parser is paired with the \texttt{validate} driver, it performs the best when it evaluates its parsing functions lazily. If \texttt{Aeson} is paired with the \texttt{convert} driver, it performs the best when it evaluates all parsing functions eagerly. 
 
In the original \Ao{} experiment, two versions of \texttt{Aeson} were optimized. Each used one of the drivers and had the incorrect set of bangs pre-inserted to allow for room for improvement. After optimization, \Ao{} was able to correct the incorrect set of bangs by either inserting bangs in correct locations or removing bangs from incorrect locations. We were interested in seeing if \At{} preserves the same results, so we ran \At{} with identical experiment conditions to see if it preserves the correct bangs that \Ao{} produces. 

In the \preopt{} phase, \At{} identified that \texttt{Aeson}'s top hot spots were in fact located in external libraries. We followed the program's suggestions to include \texttt{Data.Attoparsec.Internal.Types} as a local file in \Ao{}'s optimization coverage. However, results showed that the inclusion of this file did not significantly improve \Ao{}'s ability to optimize \texttt{Aeson}. It is possible that external libraries and files that were suggested by the \preopt{} phase do not improve \Ao{}'s optimization runtime results, in which case it is the user's decision if they still wish to include the external files as a part of the optimization coverage. In our case, we decided to remove the newly added external file, and reran the experiment using the original setup. 


%\begin{tabular}{p{2.5cm}p{1.5cm}p{1cm}p{1.5cm}}
%\hline
%Version   & Coverage & Driver & Normalized Runtime \\
%\hline
%Original      & N/A & convert   & 1     \\
%          & N/A & validate        & 1      \\
%\Ao{}  &[\texttt{Main.hs}]      & convert     & 0.56      \\
%          &[\texttt{Main.hs}] & validate        & 0.72	\\          
%\preopt{} and \Ao{}    &  [\texttt{Main.hs}, \texttt{InternalTypes.hs}]  & convert     & 0.73     \\
 %         & [\texttt{Main.hs}, \texttt{InternalTypes.hs}]  & validate        & 0.66	\\
%\hline
%\end{tabular}


In our second run of the experiment, \Ao{} was unable to generate the correct bang patterns prior to the \preopt{} phase. Because \Ao{}'s genetic algorithm is random, every experiment will turn out slightly differently every time it is run. We were unable to use \Ao{} to reproduce the exact application-specific bang patterns for \texttt{Aeson}'s two drivers that the original paper was able to produce. Therefore, we manually inserted the correct bangs into the optimization results and resumed the bang reduction process to see if the \preopt{} phase could eliminate the useless bangs and preserve the correct bangs. We also lowered the \hotspotcost{} threshold to 0.2\% because most of the high-cost \hotspots{} were located in external libraries. 

\At{} was able to identify the key parsing function as a \hotspot{} and preserved the bang pattern in the parsing function. Therefore, the correct bang patterns produced by \Ao{} were successfully preserved in the results of \At{}. The following table illustrates the runtime and bang reduction results. 

\begin{tabular}{p{2.5cm}p{1cm}p{1.5cm}p{1.5cm}}
\hline
Version   & Driver & Normalized Runtime & No. Bangs\\
\hline
Original      & convert   & 1     & 2 \\
          & validate        & 1     &  4\\
\Ao{}       & convert     & 0.91     & 46\\
          & validate        & 0.86	& 93\\
\At{}       & convert     & 0.93     &   7 \\
          & validate        & 0.85 & 2	\\
\hline
\end{tabular}

\section{Related Work and Future Work}

\subsection{\Ao{} and Other Methods of Removing Laziness}

The current strictness analyzer in GHC uses backward abstract
interpretation to identify locations that can be eagerly
evaluated. The analysis is approximate because the analysis is
static. The analysis also conservatively binds locations as strict
only if it can guarantee program termination because it is a part of
the compiler. Autobahn has the advantage of both being dynamic and not
needing to guarantee termination on all inputs as it is not a a part
of the compiler. Instead, it allows users to decide the safety of
suggested strictness annotations based on the intended application of
the program. Other approaches to reduce laziness include Strict
Haskell~\cite{strict-haskell}, which allows users to make entire modules strict rather than
lazy by default using the -XStrict and -XStrictData language
pragmas. Chang and Felleisen starts with a program written in a strict
language, and inserts laziness annotations into it using dynamic
profiling. It would be interesting to see if Chang and Felleisen's
method could be applied to introduce laziness to Strict Haskell
programs.

\subsection{\At{} Improvements}
For future developments, it would be worth exploring the additional use of heap profiles to locate hot spots instead of solely using time and allocation profiles. We have occasionally seen programs in our experiments that may be more accurately profiled by heap usage instead of time and allocation profiles. Although it may be difficult to predict which programs' \hotspots{} are more prominently associated with heap usage, \At{} could run both profiling systems for a program and compare the profiles before selecting a more suitable one to proceed with. Another potential improvement is implementing thresholds that automatically adjust themselves based on profiling results. Our experiments show that the ideal values for \hotspotcost{} and \absim{} thresholds vary by program to program. Adopting more flexible thresholds that automatically adjust themselves after inspecting the profile in the \preopt{} phase might yield better results than using set values or asking users to provide them.


\section{Conclusion}

Laziness is a double edged sword: While it provides many benefits,
excessive laziness often causes poor performance. Strictness
annotations allow programmers to force eager evaluation, but its use
is limited to programmers with extensive experience and high levels of
expertise. Autobahn uses a genetic algorithm to automatically infer
annotations for better program performance, but it often suggests too
many bangs for users to inspect. We have built Autobahn 2.0, which
uses GHC profiling feedback to perform search space manipulation to
improve the efficiency of genetic algorithms, and eliminates bangs
based on their associated performance costs. On average, experiments
show that \At{} was able to reduce 93\% of generated bangs while
only compromising 15.7\% runtime in the NoFib benchmark.

\bibliographystyle{abbrvnat}
\bibliography{autobahn}

\end{document}
