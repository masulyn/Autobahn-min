

AESON

5-13, ran both with clean InternalTypes.hs file, only optimized Main
5-4, ran both with clean InternalTypes.hs file, optimized both InternalTypes and Main

json, 

comment in and out A.Done, call json’, one with bangs and one without

json’, bang before s in s <- A.anyWord8 *> jstring_ and n in n <- scientific

how did you calculate 24 per 100 LOC



with value’ bangs	with InternalTypes bangs	convert      objects.json    14.823898s
without value’ bangs	with InternalTypes bangs	convert       objects.json     12.087662
with value’ bangs	with InternalTypes bangs	convert      chicagoezma-pppn.json    14.433711s
without value’ bangs	with InternalTypes bangs	convert       chicagoezma-pppn.json     18.096124

with value’ bangs	with InternalTypes bangs	convert      objects.json    17.372876s
without value’ bangs	with InternalTypes bangs	convert       objects.json     12.087662

with value’ bangs	with InternalTypes bangs	validate      objects.json    7.590134s
without value’ bangs	with InternalTypes bangs	validate       objects.json     14.077775
with value’ bangs	with InternalTypes bangs	validate      chicagoezma-pppn.json    22.061136s
without value’ bangs	with InternalTypes bangs	validate       chicagoezma-pppn.json     19.475297







NOFIB GRAPHS

explain why 49 benchmarks, account for missing benchmarks


autobahn fail vs fluctuation in measurement when minimizer calculated it


Section 5.2.  Explain precisely how you chose these 20 benchmark
programs. ===> FIXED

Section 5.2.  Explain why sometimes Autobahn 1.0 + pre-search
sometimes gets better performance. ===> FIXED (Added to 5.2)


Figure 1. Eliminate the beigh number next to the labels for boyer2 and
cacheprof.  ===> I think this no longer appears? not sure what the number text refers to.

Do you explain the difference between genes and bangs somewhere? ===> "In the algorithm, any program source location where a bang may be added is represented as a gene that can be turned on or off." in 2.2, let me know if it's unclear.

Figure 2. Point out in the caption that the vertical axes is on a log
scale. ===> FIXED

Section 5.3 "that's" --> "whose" ===> FIXED

Section 5.4: ===> FIXED
"validate simply checks if the file is written in valid JSON syntax,
and convert actually converts file input into a Haskell data
structure." Don't start sentences with code fragments, math, or
symbols.  In this case you can reword to something like
"The validate driver program simply checks if the file is written in valid JSON syntax,
and convert actually converts file input into a Haskell data
structure." 

Section 5.4: ===> FIXED
 What conclusions do you draw from the experimental results?
 How many bangs were in each of the programs under the different
 optimizers?

Section 5.5:
 How many programs were not successfully optimized?  Ie, how many did
 you try it on? ===> TODO need to gather data for this.

 "quite significant"  --> be more quantitative.  How much is quite
 significant? ===> TODO need to gather data for this.

anna and fluid are type set in italics in this section; json and value
are in normal font in the table in section 5.4, but in the text they
are in a sans serif font.  You should pick a convention for program
names and then use it consistently everywhere.  ====> FIXED

Explain what 1.0 and 2.0 error codes are.  Better yet, introduce
semantically meaningful names for these conditions and use those
instead. ====> FIXED. Not sure where to define these error codes but put them in 5.1 for now.

Figure 4: Why doesn't treejoin have a post-search runtime?

Section 5.5: Discuss the issue of not all runs finish successfully.  ===> TODO

Status of Section 5.6? ==> FIXED

Section 5.7.  In the third entry for Autobahn 1.0, should it be 500
instead of 100?   =====> FIXED

Discuss why reducing the search space is not reducing the running time
somewhere. ===> Added in 3.4.

Figure 6.  The runtime of anna under 1.0 isn't really 2 times the
original runtime; the 1.0 runtime is the same as the original because
Autobahn 1.0 detects the issue.  We should find a different way to
mark what happens in that case that doesn't imply the wrong resulting
running time. ===> Added error codes to represent 2.0 and 1.0 runtimes, not sure if it's clear enough?

Why the different number of benchmarks for each graph?  We're risking
looking like we are only reporting favorable results. ===> Added explanations

What's the difference between figures 7&8 and figures 9&10?  Ie, which
benchmarks appear in which figures and why? ==> FIXED



Remove generated files from git repository:
HS2018.log
HS2018.aux
HS2018.pdf
HS2018.out

********************
Section 1.3. ===> FIXED
Say something about GHC's strictness analyzer.  Do we have data on how many bangs GHC can determine are safe?  We should point out that GHC will already be applying optimizations to any program point where its strictness analyzer can prove inserting a bang would be safe.  This analysis is necessarily conservative and so misses some opportunities.  These bangs are one source of Autohbahn's performance improvements.  The other is bangs that are not always safe, but are safe on the inputs that programmers care about.  For example, factorial diverges if given a negative number, but the expectation is that it will only be called with positive numbers. 

Section 1.4 ===> FIXED
What about "pre-search" and "post-search" phases instead of "pre-optimization" and "post-optimization" phases?  The optimization term seems confusing because in some sense all the phases are doing optimizations of various kinds.

"After Ao runs"  --> "After Autobahn 1.0 runs" ? ==> FIXED

"produce far fewer bangs" --> how many fewer? ==> FIXED

1.5 ==> FIXED
Fill out.
What summary results? 

3 "Idea" --> "Autobahn 2.0" ==> FIXED

Section 4.2 ==> FIXED (I meant it would alert the user and continue to optimize as normal.)
"If external libraries could be added to Autobahn’s coverage to potentially boost optimization performance, the pre-optimization phase would alert the user and continue execution."   What does it mean to continue execution in this case?

Section 4.3 ==> They were representing genes in a chromosome, and the modified vector represents both genes and their location in a chromosome. Modified wording in an attempt to clarify that. 
What were Autobahn's bit vectors representing?  Nodes in the AST?  how did that work?

Section 4.4 ===> FIXED (clarified wording)
"We wanted to support the optimization of these files in Autobahn 2.0, so we manually removed those misidentifid genes."  Say more about this.  You manually removed these genes from what?  How would that work if someone else were using Autobahn 2.0?  Would they have to manually remove genes?

"Autobahn 2.0 successfully uses this method to avoid inserting bangs into illegal locations after being misguided by the parser and expand the types of files we can optimize to those including instance declarations as well."  This sentence is unclear because it puts two different ideas together without clearly explaining the relationship: 1) avoiding putting bangs into illegal positions 2) expanding the set of bangs through profiling.  ==> FIXED (clarified wording)

Do you explain somewhere that the file is the unit of user-control for bang placement? ===> Yes, end of 3.3.

Make captions more self-contained, as we've discussed previously. ===> FIXED


STILL NEED TO:

aut-post complete fails:
tree join

aut complete fails:
anna
fluid
fulsom
symalg

pap complete fails:
awards
callback001
callback002
exp3_8
fulsom
mutstore2
threads007
x2n1

pap complete success:
pap partial success:
pap complete fail: 8 


pap aut2 summary: runtime: 517.2 (mean 0.862) bangs: 2869.1 (mean 4.78)

aut summary: runtime: 447.15 (mean 0.745) bangs: 41078 (mean 68.46)

post-aut summary: runtime: 594.682 (mean 0.991) bangs: 2512 (mean 4.18)




minimizing bangs future work




SUMMARY

summary statistics needs to include autobahn 1.0 and 2.0 results across all succeeded and failed benchmarks


GRAPHS

discuss callback001 and other outliers


number of failed benchmarks decreased in aut 2.0. fluctuation could make a difference ,pre phase didn't have an effect on them

logscale starts from 0.25, which makes lcss look like it doesn't have a runtime (Add note to point out that left hand axis starts at 0.25)


