% Chapter 1

\chapter{Introduction} % Main chapter title

\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------

\section{Too Many Strictness Annotations In Optimized Programs}

Meet Pat and Chris. They recently implemented a Haskell program
that they were incredibly excited about before they realized that the program
ran too slowly. Pat and Chris' performance bug was a result of \textit{thunk leaks}, in which 
Haskell stores unevaluated expressions as a part of its \textit{lazy evaluation
strategy}. To counteract thunk leaks, Haskell allows users to insert \textit{strictness
annotations} to enforce strict evaluation at various program points. After some research,
Pat and Chris decided to optimize their program using \Ao{}, a Haskell optimizer
that uses genetic algorithms to automatically infer strictness annotations.

Pat and Chris provided \Ao{} with representative input to optimize their program with
and allowed the optimizer to finish running overnight. In the morning, they were
delighted to find that their program indeed ran much faster on the provided input
than it originally did. But Pat and Chris wanted their program to run faster not
only on representative input, but on all types of input that it might accept.
Most importantly, Pat and Chris needed to ensure that their program at the very least
ran safely, without non-termination or other incorrect behaviors, on all types of input.

Since only Pat and Chris know what other types of input their program might accept,
they are tasked with manually inspecting every strictness annotation that \Ao{} produced
to make sure that all annotations will produce safe behavior on all inputs. But they
soon realize that \Ao{} produced hundreds of annotations, far too many for manual inspection.
At this point, Pat and Chris are faced with two choices: spend a long time examining each
strictness annotation or cope with the unpredictability of running their optimized and
uninspected program that may produce dangerous behavior on certain types of input.

%----------------------------------------------------------------------------------------

\section{My Thesis}

In this thesis, I demonstrate that we can dramatically reduce the number of
strictness annotations generated by \Ao{} using the Haskell GHC compiler's
profiling tool. I have implemented an improved version of the optimizer
called \At{}, which minimizes generated strictness annotations while
maintaining performance of the optimized program. When evaluated on the NoFib
benchmark, \At{} reduced the number of inferred bangs by 90.2\% on average,
while only degrading program performance by 15.7\% compared with the
performance produced by \Ao{}. In a case study on a garbage collection simulator,
\At{} eliminated 81.8\% of the recommended bangs,  with the
same 15.7\% optimization degradation when compared with \Ao{}.

Apart from specifying a few additional optimization parameters, users
do not need to do much extra work to use \At{}. \At{} also does not impose any
significant additional time costs on top of what \Ao{} already demands. Similar
to running \Ao{}, users will still need to run \At{} over night as the
genetic algorithm typically runs for about 100 times longer than the program
being optimized. For this reason, Pat and Chris should continue to run
\At{} at the end of their deployment cycle after their program has been fully
implemented. However, users will typically need to spend much less time manually inspecting the minimal strictness annotations generated
by \At{}.

%----------------------------------------------------------------------------------------

\section{Lazy Evaluation}

To understand the solution to Pat and Chris' dilemma, it is useful to
first understand the root of their performance problem - Haskell's lazy
evaluation strategy. Under lazy evaluation, expressions are
only evaluated when their values are needed. Every unevaluated
expression is stored in a \textit{thunk}, and its evaluation is
delayed until another expression demands the value of the current
one~\cite{PeytonJones89}. Most of the time lazy evaluation is beneficial -
it supports modularity, can improve program efficiency, and enables
the use of infinite data structures~\cite{Hughes89}.

While lazy evaluation reaps many benefits, it can also cause serious
performance problems in both time and space when large amounts of
memory are allocated to thunks~\cite{Jones94,Santos98,Ennals03}. This problem is
also why Pat and Chris' program runs much slower than anticipated.
Reducing the number of thunks created at runtime is an important
optimization in the GHC compiler, which uses a backward static
analysis~\cite{Sergey14} to statically find expressions that the
compiler can safely evaluate immediately rather than converting into
thunks. Because this analysis is part of the GHC compiler, it must be
conservative, eliminating only thunks that it can prove will not
affect the program semantics when given any possible input.
Unfortunately, this conservative analysis is not always sufficient, in
which case programmers can manually insert strictness annotations such
as \textit{bang patterns}~\cite{bang}, which instruct the compiler to
immediately evaluate the corresponding expression regardless of
whether the strictness analysis determines it is safe to do so. These
manual strictness annotations can significantly improve
performance~\cite[Chapter~25]{rwh}. However, programmers need to
distinguish program points that will benefit from eager evaluation
from program points that do not need to be evaluated or will not
terminate when evaluated. This task is difficult and often reserved
for expert Haskell programmers~\cite{Mitchell13}.

%----------------------------------------------------------------------------------------

\section{\Ao}

\Ao~\cite{autobahn-wang} is a tool that helps Haskell programmers
reduce their thunk usage by automatically inferring strictness
annotations. Users provide \Ao{} with an unoptimized
program, representative input, and an optional configuration file to
obtain an optimized version of the program over the course of a couple
of hours.

\Ao{} uses a genetic algorithm to randomly search for
beneficial locations to place bangs in the program. The genetic
algorithm iteratively measures the performances of a series of
candidate bang placements. Candidates that improve upon the original
program's performance are preserved, and candidates that trigger
non-termination or worsen program performance are
eliminated. Preserved candidates are then either mutated or combined to produce newer generations of candidate bang placements. When the budgeted optimization time is up, \Ao{} returns a list
of well-performing candidates, ranked by how much each candidate
improves program performance. Users can then inspect the candidate
bang placements and decide if they want to apply one of them to the
program.

%----------------------------------------------------------------------------------------

\section{\At{}}

This thesis presents \At{}, an improved version of \Ao{} that aims
to reduce the number of generated bangs.
\At{} introduces a \textit{\preopt{} phase} and
\textit{\postopt{} phase} that run before and after \Ao{},
respectively. Both phases use information from GHC's profiler to
locate and eliminate ineffective bangs.
GHC profiles are helpful in this regard because they
show the amount of runtime and memory each location in the
program is responsible for. The \preopt{} phase uses this information to adjust the set of files that
\Ao{} considers during its optimization.
Specifically, this phase instructs \Ao{} to
optimize files that contain locations that require significant
resources, skipping files that do not and
potentially adding files not originally included by the users.
After \Ao{} runs, the \postopt{} phase individually tests
each candidate bang that falls within a costly location. Bangs in costly locations that
do not significantly impact program performance are
eliminated, as well as bangs that do not fall in costly locations.   The system is parameterized, so users can manage the
tradeoff between aggressively reducing the number of bangs and
preserving performance improvements.  In our experiments, we chose to
aggressively reduce the number bangs, accepting some performance
degradation over the level of optimization provided by \Ao{}.

The contributions of this thesis are the following:
\begin{itemize}
  \item We describe how to use profiling information to automatically reduce the number
    of bangs inferred by \Ao{} while maintaining roughly the same
    level of optimization.
  \item We show that \At{} applied to the NoFib benchmark suite reduced
    the number of generated bangs by 90.2\% on average, while
    increasing the runtime of the optimized program by 15.7\% over the
    runtime of the program optimized by \Ao{} alone. We refer to this
    performance change as a 15.7\% \textit{optimization degradation}.
\cut{(2.0 runtime - 1.0 runtime)/original}
  \item We demonstrate that the \preopt{} phase removed at least
    one file from consideration in 21 of the benchmarks in the NoFib
    suite, corresponding to 35\% of the programs we considered.
    For these programs, the \preopt{} phase eliminated
    45~potential bang locations per 100~LOC, resulting in a mean bang
    reduction of 87.79\% across the entire benchmark suite.
  \item We use a microbenchmark to show that the \preopt{} phase's
    suggestions for additional files to consider can improve \Ao{}'s
    optimization results by 86.6\%.
  \item We evaluate the \postopt{} phase on the NoFib benchmarks,
    showing it can reduce the number of inferred bangs by
    93.8\% with a 33\% optimization degradation.
  \item We use \At{} in a case study to optimize the performance of
    \texttt{gcSimulator}~\cite{Ricci13}, a garbage collector
    simulator. The system reduced the number of inferred bangs by
    81.8\% with a 15.7\% optimization degradation.
\acut{
  \item We apply \At{} in a second case study to show that it can
    preserve the application-specific annotations inferred by \Ao{}
    for two different uses of the Aeson~\cite{aeson} library.}
\end{itemize}
